{
 "cells": [
  {
   "cell_type": "raw",
   "id": "7a483f3b-11c5-4cb9-9cb9-d74d0f2a9347",
   "metadata": {},
   "source": [
    "this is a copy of the \"cnn_boundingbox\" implementation -> with vgg16 pretrained conv layers\n",
    "\n",
    "bounding box regression on caltech 101 dataset.\n",
    "limited to binary classification\n",
    "dataset source: https://data.caltech.edu/records/mzrjq-6wc02\n",
    "\n",
    "dataset description:\n",
    "Pictures of objects belonging to 101 categories. About 40 to 800 images per category. Most categories have about 50 images. Collected in September 2003 by Fei-Fei Li, Marco Andreetto, and Marc'Aurelio Ranzato. The size of each image is roughly 300 x 200 pixels. We have carefully clicked outlines of each object in these pictures, these are included under the 'Annotations.tar'. There is also a MATLAB script to view the annotations, 'show_annotations.m'.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad759170-945a-40bf-bfa4-e1ae16c6f854",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader, Subset, Dataset\n",
    "from torchvision.datasets import DatasetFolder, ImageFolder\n",
    "from torchvision import transforms\n",
    "from torchvision.models import vgg16\n",
    "import random\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "709bc3f1-3e69-499d-931f-7c494c75dce3",
   "metadata": {},
   "source": [
    "# 1. preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3383bdb0-28ce-4e4d-bbcd-9b86782a08ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nplan:\\nimages and box labels are in separate files. -> we have to connect them -> create a dataset for dataloading\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "plan:\n",
    "images and box labels are in separate files. -> we have to connect them -> create a dataset for dataloading\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ceb03e57-b021-407c-8c2e-35e33b8269c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "import PIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cacaab0e-2ad5-4be1-b50e-37ea80326c82",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'__header__': b'MATLAB 5.0 MAT-file, Platform: PCWIN, Created on: Tue Dec 14 11:03:29 2004', '__version__': '1.0', '__globals__': [], 'box_coord': array([[ 30, 137,  49, 349]], dtype=uint16), 'obj_contour': array([[  8.54082661,  11.87852823,   1.86542339,   1.56199597,\n",
      "         31.60131048,  27.65675403,  23.71219758,  18.85735887,\n",
      "         18.85735887,  31.60131048,  47.68296371,  51.32409274,\n",
      "         59.51663306,  60.1234879 ,  56.78578629,  78.02570565,\n",
      "         91.07308468, 178.46018145, 179.97731855, 222.15372984,\n",
      "        225.79485887, 239.75252016, 265.84727823, 298.92086694,\n",
      "        300.13457661, 298.3140121 , 265.54385081, 264.63356855,\n",
      "        270.39868952, 268.88155242, 265.84727823, 264.02671371,\n",
      "        260.08215726, 255.83417339, 257.6547379 , 261.90272177,\n",
      "        261.90272177, 160.25453629, 160.25453629, 156.00655242,\n",
      "        155.39969758, 149.33114919, 142.04889113, 139.31804435,\n",
      "        139.92489919, 143.26260081, 136.28377016, 128.09122984,\n",
      "        124.45010081, 123.53981855, 122.93296371, 117.47127016,\n",
      "         73.77772177,  68.61945565,  44.95211694,  38.27671371,\n",
      "         36.15272177,   8.23739919,   8.54082661],\n",
      "       [ 75.89163306,  62.54082661,  59.80997984,  56.77570565,\n",
      "         51.01058468,  35.23235887,  11.26159274,  10.35131048,\n",
      "          6.10332661,   1.2484879 ,   3.37247984,  12.171875  ,\n",
      "         12.47530242,  19.15070565,  18.54385081,  47.97631048,\n",
      "         51.3140121 ,  49.19002016,  44.03175403,  45.54889113,\n",
      "         51.3140121 ,  55.25856855,  72.85735887,  85.60131048,\n",
      "         90.15272177,  90.75957661,  92.58014113,  94.09727823,\n",
      "         95.3109879 , 100.77268145, 101.37953629, 104.7172379 ,\n",
      "        105.93094758, 102.28981855,  95.3109879 ,  94.70413306,\n",
      "         91.66985887,  90.15272177,  97.13155242,  97.73840726,\n",
      "        102.89667339, 105.32409274, 104.7172379 ,  97.43497984,\n",
      "         93.79385081,  91.36643145,  88.9390121 ,  88.33215726,\n",
      "         92.88356855,  92.58014113,  88.63558468,  86.81502016,\n",
      "         83.78074597,  85.9047379 ,  82.87046371,  85.60131048,\n",
      "         80.13961694,  75.28477823,  75.58820565]])}\n"
     ]
    }
   ],
   "source": [
    "# insights on matfile\n",
    "\n",
    "mat_file = scipy.io.loadmat(\"./caltech-101/Annotations/Airplanes_Side_2/annotation_0001.mat\")\n",
    "print(mat_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1579aaed-3fd1-4046-91ad-c498c0e7723c",
   "metadata": {},
   "source": [
    "## 2. build the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "124fa595-cb32-42b3-8d2a-8ac9487fd184",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "vgg16 requires 224x224 px input images.\n",
    "7x7x512 on the final conv layer. -> 25088\n",
    "we chop up the heads, and use only the conv layers.\n",
    "\"\"\"\n",
    "\n",
    "# load and freeze vgg feature maps.\n",
    "vgg_model = vgg16(weights=\"DEFAULT\").features\n",
    "for param in vgg_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "\n",
    "class Network(nn.Module):\n",
    "\n",
    "    def __init__(self, n_classes, seed=101):\n",
    "        super().__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.features = nn.Sequential(\n",
    "            vgg_model\n",
    "        )\n",
    "\n",
    "        self.n_flattened_neurons = 25088\n",
    "        \n",
    "        # classification head\n",
    "        self.class_head = nn.Sequential(\n",
    "            nn.Linear(self.n_flattened_neurons, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, n_classes),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        # localization head\n",
    "        self.localize_head = nn.Sequential(\n",
    "            nn.Linear(self.n_flattened_neurons, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 4),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, data):\n",
    "        signal = self.features(data)\n",
    "        signal = self.flatten(signal)\n",
    "        #print(signal.size(1))\n",
    "        classes = self.class_head(signal)\n",
    "        bounding_boxes = self.localize_head(signal)\n",
    "        return classes, bounding_boxes"
   ]
  },
  {
   "cell_type": "raw",
   "id": "04dcf5e5-6d9b-4528-b28e-ab2682a963f2",
   "metadata": {},
   "source": [
    "# test\n",
    "\n",
    "testwork = Network(1)\n",
    "# image_0031 size=402x236\n",
    "test_img = PIL.Image.open(\"./caltech-101/subset/images/airplanes/image_0031.jpg\")\n",
    "transformer = transforms.Compose([transforms.Resize((224, 224)), transforms.ToTensor()])\n",
    "test_img_resized = transformer(test_img)\n",
    "print(testwork(test_img_resized.unsqueeze(0)))\n",
    "# check after resizing\n",
    "transformer = transforms.ToPILImage()\n",
    "reconverted_img = transformer(test_img_resized)\n",
    "reconverted_img.show()\n",
    "\n",
    "# image_0032 size=392x147 -> needs padding \n",
    "test_img2 = PIL.Image.open(\"./caltech-101/subset/images/airplanes/image_0032.jpg\")\n",
    "transformer = transforms.Compose([transforms.Resize((224, 224)), transforms.ToTensor()])\n",
    "test_img2_resized = transformer(test_img2)\n",
    "print(testwork(test_img2_resized.unsqueeze(0)))\n",
    "# check after resizing\n",
    "transformer = transforms.ToPILImage()\n",
    "reconverted_img = transformer(test_img2_resized)\n",
    "reconverted_img.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9cb346-7673-427a-8901-28ea48b69183",
   "metadata": {},
   "source": [
    "## 3. preprocess dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8197cef6-02cd-4f14-a2a1-c9dbbb5a41f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 28 116  71 326]\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=405x140 at 0x1B341390F10>\n",
      "pil:  405 140\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nrectangle looks like: y1, y2, x1, x2 ->  upper left and bottom right corner\\n\\n  x--------\\n |        |\\n |        |\\n  --------x\\n  \\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# insights on image and box\n",
    "\n",
    "mat_file = scipy.io.loadmat(\"./caltech-101/Annotations/Airplanes_Side_2/annotation_0056.mat\")\n",
    "coords = mat_file[\"box_coord\"].squeeze()\n",
    "print(coords)\n",
    "\n",
    "# !! cv returns (h, w) <-> pil returns (w, h)\n",
    "image = PIL.Image.open(\"./caltech-101/subset/images/airplanes/image_0056.jpg\")\n",
    "print(image)\n",
    "w, h = image.size\n",
    "print(\"pil: \", w, h)\n",
    "#image.show()\n",
    "\n",
    "draw_obj = PIL.ImageDraw.Draw(image)\n",
    "draw_obj.rectangle([coords[2], coords[0], coords[3], coords[1]], outline=\"red\")\n",
    "image.show()\n",
    "\n",
    "\"\"\"\n",
    "rectangle looks like: y1, y2, x1, x2 ->  upper left and bottom right corner\n",
    "\n",
    "  x--------\n",
    " |        |\n",
    " |        |\n",
    "  --------x\n",
    "  \n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c319e837-476e-4580-896f-235e3579dd22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# image preprocessing\n",
    "\n",
    "# The size of each image is roughly 300 x 200 pixels. (according to dataset description) \n",
    "def preprocess_image(image, target_size=224, return_scale_and_pad=True):\n",
    "    \"\"\"\n",
    "    Resize while keeping aspect ratio and adding padding to match target_size.\n",
    "    return_scale_and_pad: useful info for bboxes, if they have to be corrected.\n",
    "    returns: (image, scale, tuple(padding))\n",
    "    \"\"\"\n",
    "    w, h = image.size\n",
    "    \n",
    "    # Compute the new size while keeping the aspect ratio\n",
    "    scale = target_size / max(w, h)  # Scaling factor\n",
    "    new_w, new_h = int(w * scale), int(h * scale)\n",
    "    \n",
    "    # Resize while preserving aspect ratio\n",
    "    resize_transform = transforms.Resize((new_h, new_w), interpolation=transforms.InterpolationMode.BILINEAR)\n",
    "    image = resize_transform(image)\n",
    "\n",
    "    # Compute padding values (left, top, right, bottom)\n",
    "    pad_w = target_size - new_w\n",
    "    pad_h = target_size - new_h\n",
    "    padding = (pad_w // 2, pad_h // 2, pad_w - pad_w // 2, pad_h - pad_h // 2)\n",
    "\n",
    "    # Apply padding + transform to tensor\n",
    "    # transforms.Pad expects left,top,right,bottom IN ORDER\n",
    "    pad_transform = transforms.Compose([transforms.Pad(padding, fill=255), transforms.ToTensor()])\n",
    "    image = pad_transform(image)\n",
    "\n",
    "    if return_scale_and_pad == True:\n",
    "        return image, scale, padding\n",
    "    else:\n",
    "        return image\n",
    "\n",
    "\n",
    "# the bounding box in the matfile correspond to -> upper left and bottom right corner\n",
    "# like: y1, y2, x1, x2\n",
    "# see above the insights\n",
    "def preprocess_bounding_box(coords, padded_img, scaling_factor, padding, reverse=False):\n",
    "    # padded img coming as a tensor -> must retransform\n",
    "    transformer = transforms.ToPILImage()\n",
    "    reconverted_img = transformer(padded_img)\n",
    "    padded_img = reconverted_img\n",
    "    padded_width, padded_height = padded_img.size\n",
    "    pad_w_left, pad_h_top, pad_w_right, pad_h_bottom = padding      \n",
    "    \n",
    "    if reverse == False:\n",
    "        y1, y2, x1, x2 = coords\n",
    "        # scale the original coords to get the new coords\n",
    "        x1 = x1 * scaling_factor\n",
    "        x2 = x2 * scaling_factor\n",
    "        y1 = y1 * scaling_factor\n",
    "        y2 = y2 * scaling_factor\n",
    "    \n",
    "        # compute new coords with padding + normalize (0 to 1)\n",
    "        x1 = (x1 + pad_w_left) / padded_width\n",
    "        x2 = (x2 + pad_w_right) / padded_width\n",
    "        y1 = (y1 + pad_h_top) / padded_height\n",
    "        y2 = (y2 + pad_h_bottom) / padded_height\n",
    "    \n",
    "    else:\n",
    "        # ! coords order is different\n",
    "        x1, y1, x2, y2 = coords\n",
    "        x1 = x1 * padded_width\n",
    "        x2 = x2 * padded_width\n",
    "        y1 = y1 * padded_height\n",
    "        y2 = y2 * padded_height\n",
    "\n",
    "        x1 = x1 - pad_w_left\n",
    "        x2 = x2 - pad_w_right\n",
    "        y1 = y1 - pad_h_top \n",
    "        y2 = y2 - pad_h_bottom\n",
    "\n",
    "        x1 = x1 / scaling_factor\n",
    "        x2 = x2 / scaling_factor\n",
    "        y1 = y1 / scaling_factor\n",
    "        y2 = y2 / scaling_factor\n",
    "\n",
    "    # reorder to draw rectangle later\n",
    "    coords = np.array((x1, y1, x2, y2))\n",
    "    coords = torch.from_numpy(coords).float()\n",
    "    \n",
    "    return coords\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d25a2e2a-3b0d-42ee-9660-07349050744b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 29 133  49 339]\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=392x159 at 0x1B341410F40>\n",
      "pil:  392 159\n",
      "[0.125      0.37308672 0.8647959  0.63839287]\n",
      "28.0 83.57142543792725 193.71428680419922 143.00000190734863\n"
     ]
    }
   ],
   "source": [
    "# check if the new preprocessing works correctly\n",
    "\n",
    "mat_file = scipy.io.loadmat(\"./caltech-101/subset/annotations/airplanes/annotation_0033.mat\")\n",
    "coords = mat_file[\"box_coord\"].squeeze()\n",
    "print(coords)\n",
    "\n",
    "# !! cv returns (h, w) <-> pil returns (w, h)\n",
    "image = PIL.Image.open(\"./caltech-101/subset/images/airplanes/image_0033.jpg\")\n",
    "print(image)\n",
    "w, h = image.size\n",
    "print(\"pil: \", w, h)\n",
    "#image.show()\n",
    "\n",
    "#draw_obj = PIL.ImageDraw.Draw(image)\n",
    "#draw_obj.rectangle([coords[2], coords[0], coords[3], coords[1]], outline=\"red\")\n",
    "#image.show()\n",
    "\n",
    "\n",
    "# preprocess then show:\n",
    "padded_image = preprocess_image(image)\n",
    "boxes = preprocess_bounding_box(coords, padded_image[0], padded_image[1], padded_image[2])\n",
    "# reconvert to see the test\n",
    "transformer = transforms.ToPILImage()\n",
    "reconverted_img = transformer(padded_image[0])\n",
    "reconverted_width, reconverted_height = reconverted_img.size\n",
    "\n",
    "boxes = boxes.numpy()\n",
    "print(boxes)\n",
    "x1 = boxes[0] * reconverted_width\n",
    "y1 = boxes[1] * reconverted_height\n",
    "x2 = boxes[2] * reconverted_width\n",
    "y2 = boxes[3] * reconverted_height\n",
    "\n",
    "print(x1, y1, x2, y2)\n",
    "\n",
    "draw_obj = PIL.ImageDraw.Draw(reconverted_img)\n",
    "draw_obj.rectangle([x1, y1, x2, y2], outline=\"red\")\n",
    "reconverted_img.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0867fe80-80d5-43d4-8ed6-351e3a6843b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ce067f-7ee6-4452-b1dd-5dfad26fb70b",
   "metadata": {},
   "source": [
    "## 4. init hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c2dff169-ecb2-4740-ae7c-d9f4be810df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "#learning_rate = 0.001\n",
    "learning_rate = 0.0001\n",
    "n_epochs = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c1f8fc-741a-43f1-9dd5-2bdd5efa824c",
   "metadata": {},
   "source": [
    "## 5. create dataset + dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ec696b61-d88c-458e-9bfc-6fedcc1625bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create custom dataset loader to connect the image + label + bounding box coords\n",
    "import warnings\n",
    "\n",
    "\n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    root_folder: relative path\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, root_folder=None, transform_img=None, transform_annot=None):\n",
    "\n",
    "        self.root_folder = root_folder\n",
    "        self.transform_img = transform_img\n",
    "        self.transform_annot = transform_annot\n",
    "        self.image_paths, self.class_label, self.annotation_paths = self.__get_paths_and_classes()\n",
    "\n",
    "\n",
    "    def __get_paths_and_classes(self):\n",
    "        if self.root_folder:\n",
    "            print(\"CustomDataset initializing...\")\n",
    "            image_paths = []\n",
    "            annotation_paths = []\n",
    "            classes_to_label = []\n",
    "    \n",
    "    \n",
    "            images_folder = os.path.join(self.root_folder, \"images\")\n",
    "            class_label = -1\n",
    "            for one_class in os.listdir(images_folder):\n",
    "                class_path = os.path.join(images_folder, one_class)\n",
    "                class_label += 1\n",
    "                print(\"img_classes path: \", class_path)\n",
    "                if os.path.isdir(class_path):\n",
    "                    n_images = 0\n",
    "                    for image in os.listdir(class_path):\n",
    "                        image_path = os.path.join(class_path, image)\n",
    "                        image_paths.append(image_path)\n",
    "                        n_images += 1\n",
    "                        # int\n",
    "                        classes_to_label.append(class_label)\n",
    "                        #print(image_path)\n",
    "                print(\"image appended: {}\".format(n_images))\n",
    "    \n",
    "            annotation_folder = os.path.join(self.root_folder, \"annotations\")\n",
    "            for one_class in os.listdir(annotation_folder):\n",
    "                class_path = os.path.join(annotation_folder, one_class)\n",
    "                print(\"annot_classes path: \", class_path)\n",
    "                if os.path.isdir(class_path):\n",
    "                    n_annotations = 0\n",
    "                    for annotation in os.listdir(class_path):\n",
    "                        annotation_path = os.path.join(class_path, annotation)\n",
    "                        annotation_paths.append(annotation_path)\n",
    "                        n_annotations += 1\n",
    "                        #print(annotation_path)\n",
    "                    print(\"annotation appended: {}\".format(n_annotations))\n",
    "    \n",
    "            # sort + np.array: os.listdir badly shuffles the names + annotations\n",
    "            image_paths.sort()\n",
    "            annotation_paths.sort()\n",
    "            image_paths = np.array(image_paths)\n",
    "            annotation_paths = np.array(annotation_paths)\n",
    "            print(\"CustomDataset initialized \\n\")\n",
    "            \n",
    "            return image_paths, classes_to_label, annotation_paths\n",
    "        else:\n",
    "            warnings.warn(\"No root folder specified. Ignore this warning, if you used '.train_test_split()'\")\n",
    "            return None, None, None\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        current_img_path = self.image_paths[idx]\n",
    "        current_annot_path = self.annotation_paths[idx]\n",
    "        current_class_label = self.class_label[idx]\n",
    "\n",
    "        # i have found colorless image -> must convert to 3d channel\n",
    "        current_img = PIL.Image.open(current_img_path).convert(\"RGB\")\n",
    "        current_annot = scipy.io.loadmat(current_annot_path)\n",
    "        current_annot = current_annot[\"box_coord\"].squeeze()\n",
    "        current_class_label = torch.tensor(current_class_label).float().unsqueeze(0)\n",
    "\n",
    "        # apply transformation\n",
    "        if self.transform_img and self.transform_annot:\n",
    "            current_img, current_annot = self.apply_transformation(current_img, current_annot)\n",
    "\n",
    "        #print(current_img_path)\n",
    "        return current_img, current_class_label, current_annot\n",
    "\n",
    "    \n",
    "    def apply_transformation(self, img, annot):\n",
    "        img, scale, padding = self.transform_img(img)\n",
    "        annot = self.transform_annot(annot, img, scale, padding)\n",
    "        return img, annot\n",
    "\n",
    "    \n",
    "    def train_test_split(self, test_size):\n",
    "        \"\"\"\n",
    "        manual, basic split. random selection with no respect to class sizes.\n",
    "        \n",
    "        returns: new CustomDataset obj as test set.\n",
    "        note: returns with the same transforms as arguments on the original dataset\n",
    "        \n",
    "        generate random indexes, put those paths to the test_set, delete them from original paths\n",
    "        \"\"\"\n",
    "        total_test_size = int(len(self.image_paths) * test_size)\n",
    "        print(\"total test size: \", total_test_size)\n",
    "        rand_indexes = np.random.randint(0, len(self.image_paths), size=total_test_size)\n",
    "        rand_indexes = set(rand_indexes)\n",
    "        while len(rand_indexes) != total_test_size:\n",
    "            rand_indexes.add(np.random.randint(0, len(self.image_paths)))\n",
    "        # transform back to list for numpy to handle\n",
    "        rand_indexes = list(rand_indexes)\n",
    "        \n",
    "        test_image_paths = []\n",
    "        test_class_label = []\n",
    "        test_annotation_paths = []\n",
    "        for one_idx in rand_indexes:\n",
    "            test_image_paths.append(self.image_paths[one_idx])\n",
    "            test_class_label.append(self.class_label[one_idx])\n",
    "            test_annotation_paths.append(self.annotation_paths[one_idx])\n",
    "\n",
    "        self.image_paths = np.delete(self.image_paths, rand_indexes)\n",
    "        self.class_label = np.delete(self.class_label, rand_indexes)\n",
    "        self.annotation_paths = np.delete(self.annotation_paths, rand_indexes)\n",
    "\n",
    "        \"\"\"\n",
    "        OLD\n",
    "        test_set = []\n",
    "        # in case of when self.image_paths,... etc are simple lists. not np arrays\n",
    "        for elem in test_set:\n",
    "            if elem[0] in self.image_paths:\n",
    "                self.image_paths.remove(elem[0])\n",
    "            if elem[1] in self.class_label:\n",
    "                self.class_label.remove(elem[1])\n",
    "            if elem[2] in self.annotation_paths:\n",
    "                self.annotation_paths.remove(elem[2])\n",
    "        \"\"\"\n",
    "        new_dataset_obj = CustomDataset(root_folder=None, transform_img=self.transform_img, transform_annot=self.transform_annot)\n",
    "        new_dataset_obj.image_paths = test_image_paths\n",
    "        new_dataset_obj.class_label = test_class_label\n",
    "        new_dataset_obj.annotation_paths = test_annotation_paths\n",
    "        \n",
    "        return new_dataset_obj\n",
    "        \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "21962a4b-d588-486c-b851-0d1a5edcfa55",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CustomDataset initializing...\n",
      "img_classes path:  ./caltech-101/subset/images\\airplanes\n",
      "image appended: 800\n",
      "img_classes path:  ./caltech-101/subset/images\\Motorbikes\n",
      "image appended: 798\n",
      "annot_classes path:  ./caltech-101/subset/annotations\\airplanes\n",
      "annotation appended: 800\n",
      "annot_classes path:  ./caltech-101/subset/annotations\\Motorbikes\n",
      "annotation appended: 798\n",
      "CustomDataset initialized \n",
      "\n",
      "./caltech-101/subset/images\\airplanes\\image_0033.jpg\n",
      "1\n",
      "./caltech-101/subset/annotations\\airplanes\\annotation_0033.mat\n",
      "\n",
      "\n",
      "./caltech-101/subset/images\\airplanes\\image_0032.jpg\n",
      "1\n",
      "./caltech-101/subset/annotations\\airplanes\\annotation_0032.mat\n",
      "\n",
      "\n",
      "first elem: \n",
      " (<PIL.Image.Image image mode=RGB size=262x161 at 0x1B34144E0E0>, tensor([0.]), array([ 19, 141,  31, 233], dtype=uint8)) \n",
      " ----------------------- \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# test dataset\n",
    "\n",
    "root_path = \"./caltech-101/subset/\"\n",
    "\n",
    "test_c_dataset = CustomDataset(root_path)\n",
    "\n",
    "print(test_c_dataset.image_paths[830])\n",
    "print(test_c_dataset.class_label[830])\n",
    "print(test_c_dataset.annotation_paths[830])\n",
    "print(\"\\n\")\n",
    "print(test_c_dataset.image_paths[829])\n",
    "print(test_c_dataset.class_label[829])\n",
    "print(test_c_dataset.annotation_paths[829])\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"first elem: \\n\", test_c_dataset[0], \"\\n\", \"-----------------------\",  \"\\n\")\n",
    "test_c_dataset[0][0].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7ce8b9a7-0b47-412a-bd22-71e5d941ff71",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CustomDataset initializing...\n",
      "img_classes path:  ./caltech-101/subset/images\\airplanes\n",
      "image appended: 800\n",
      "img_classes path:  ./caltech-101/subset/images\\Motorbikes\n",
      "image appended: 798\n",
      "annot_classes path:  ./caltech-101/subset/annotations\\airplanes\n",
      "annotation appended: 800\n",
      "annot_classes path:  ./caltech-101/subset/annotations\\Motorbikes\n",
      "annotation appended: 798\n",
      "CustomDataset initialized \n",
      "\n",
      "total test size:  159\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\neeko\\AppData\\Local\\Temp\\ipykernel_14608\\1525864422.py:65: UserWarning: No root folder specified. Ignore this warning, if you used '.train_test_split()'\n",
      "  warnings.warn(\"No root folder specified. Ignore this warning, if you used '.train_test_split()'\")\n"
     ]
    }
   ],
   "source": [
    "root_path = \"./caltech-101/subset/\"\n",
    "\n",
    "# !! got no idea how these function passes not results in error...\n",
    "train_set = CustomDataset(root_path, transform_img=preprocess_image, transform_annot=preprocess_bounding_box)\n",
    "test_set = train_set.train_test_split(0.1)\n",
    "loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "48736b3b-73c2-4f69-96ac-52d8f2ad6c92",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len train set:  1439\n",
      "len test set:  159\n",
      "\n",
      "./caltech-101/subset/images\\airplanes\\image_0087.jpg\n",
      "35. elem: \n",
      " (tensor([[[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.]]]), tensor([0.]), tensor([0.1221, 0.3111, 0.8779, 0.6813])) \n",
      " ----------------------- \n",
      "\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1.])\n"
     ]
    }
   ],
   "source": [
    "# test loader\n",
    "\n",
    "print(\"len train set: \", len(train_set))\n",
    "print(\"len test set: \", len(test_set))\n",
    "print(\"\")\n",
    "\n",
    "print(train_set.image_paths[797])\n",
    "print(\"35. elem: \\n\", train_set[34], \"\\n\", \"-----------------------\",  \"\\n\")\n",
    "print(train_set[34][0][0][0])\n",
    "transform = transforms.ToPILImage()\n",
    "reconverted_img = transform(train_set[34][0])\n",
    "reconverted_img.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac36bddc-3f98-45eb-b618-fb6716e3531b",
   "metadata": {},
   "source": [
    "## 6. training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9edfd2e9-6289-4358-ab1b-7d99c67b2309",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 for binary classif\n",
    "brain = Network(1)\n",
    "optimizer = Adam(brain.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f77fca86-0e82-4253-8478-454a51b51dca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  1 \t loss:  0.10305125\n",
      "epoch:  2 \t loss:  0.0054028807\n",
      "epoch:  3 \t loss:  0.0011817609\n",
      "epoch:  4 \t loss:  0.0006742767\n",
      "epoch:  5 \t loss:  0.00044308222\n",
      "epoch:  6 \t loss:  0.00031977816\n",
      "epoch:  7 \t loss:  0.0002435609\n",
      "epoch:  8 \t loss:  0.00019344168\n"
     ]
    }
   ],
   "source": [
    "# training\n",
    "\n",
    "\n",
    "for epoch in range(1, n_epochs+1):\n",
    "    losses = []\n",
    "    for image, label, annotation in loader:\n",
    "        pred_class, pred_box = brain(image)\n",
    "        #print(pred_class)\n",
    "        #print(pred_box)\n",
    "        #print(label)\n",
    "        loss_classif = F.binary_cross_entropy(pred_class, label)\n",
    "        loss_box = F.mse_loss(pred_box, annotation)\n",
    "\n",
    "        # total loss?\n",
    "        total_loss = loss_classif + loss_box\n",
    "        losses.append(total_loss.data)\n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(\"epoch: \", epoch, \"\\t loss: \", np.mean(losses))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d88110-e772-454d-a156-25318d62e4a8",
   "metadata": {},
   "source": [
    "## 7. save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "22f9d1a5-dee0-4be9-8973-5990493830bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_output_txt():\n",
    "    structure = Network(1)\n",
    "    txt = f\"\"\"{structure.features}, \\n\n",
    "    flattened neurons: {structure.n_flattened_neurons}, \\n \n",
    "    classification head: {structure.class_head}, \\n\n",
    "    localization head: {structure.localize_head}, \\n\n",
    "    training batch size: {batch_size}, \n",
    "    learning rate: {learning_rate},\n",
    "    epochs: {n_epochs}\n",
    "    losses: {np.mean(losses)}\"\"\"\n",
    "    return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "df13ed57-3105-46cd-83b6-ec2738f2826f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved as:  plane_bike_checkpoint_vgg16_2\n"
     ]
    }
   ],
   "source": [
    "base_output = \"outputs\"\n",
    "save_name = \"plane_bike_checkpoint_vgg16_2\"\n",
    "save_file_extension = \".pth\"\n",
    "overwrite = True\n",
    "full_path = os.path.join(base_output, save_name + save_file_extension)\n",
    "\n",
    "\n",
    "os.makedirs(base_output, exist_ok=True)\n",
    "if os.path.exists(full_path):\n",
    "    if overwrite == True:\n",
    "        torch.save(brain.state_dict(), full_path)\n",
    "        print(\"saved as: \", save_name)\n",
    "    else:\n",
    "        print(\"save failed. file already exists. to overwrite, set overwrite=True\")\n",
    "else:\n",
    "    torch.save(brain.state_dict(), full_path)\n",
    "    print(\"saved as: \", save_name)\n",
    "\n",
    "with open(base_output + \"/\" + save_name + \".txt\", \"w\", encoding=\"utf-8\") as textfile:\n",
    "    textfile.write(prep_output_txt())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f35e9a8-a910-4ced-8ec8-711ddbac1823",
   "metadata": {},
   "source": [
    "## 8. testing + accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7f4afb3e-8e3c-414e-b36a-72b19d3436d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment to load the saved one.\n",
    "\n",
    "#brain.load_state_dict(torch.load(\"plane_bike_checkpoint.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "675daf3f-60f5-45cf-b36b-d72b4382701a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing\n",
    "\n",
    "# due to my mistake and lack of separate test folder, first i did not split the data to train-test.\n",
    "# so, i downloaded images from google to test.\n",
    "# i leave this here, because the result is interesting\n",
    "\n",
    "# 1. mass pred\n",
    "test_dataset = ImageFolder(\"./caltech-101/subset/manual_testset/\", transform=preprocess_img)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "print(\"test dataset: \", test_dataset, \"\\n\")\n",
    "\n",
    "brain.eval()\n",
    "predictions = []\n",
    "with torch.no_grad():\n",
    "    for img, label in test_loader:\n",
    "        pred_class, pred_box = brain(img)\n",
    "        predictions.append({\"class true\" : int(label), \"prediction prob\" : float(pred_class)})\n",
    "\n",
    "\n",
    "print(\"class true \\t prediction \\t prediction prob\")\n",
    "for elem in predictions:\n",
    "    if elem[\"class true\"] == 1:\n",
    "        elem[\"class true\"] = \"airplane\"\n",
    "    else:\n",
    "        elem[\"class true\"] = \"motorbike\"\n",
    "    if elem[\"prediction prob\"] >= 0.5:\n",
    "        elem[\"prediction\"] = \"airplane\"\n",
    "    else:\n",
    "        elem[\"prediction\"] = \"motorbike\"\n",
    "    print(elem[\"class true\"], \"\\t\", elem[\"prediction\"], \"\\t\", elem[\"prediction prob\"])\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "\n",
    "# 2. single pred\n",
    "img_path = \"./caltech-101/subset/manual_testset/motor/motor1.jpg\"\n",
    "image = PIL.Image.open(img_path)\n",
    "image.show()\n",
    "image = preprocess_img(image)\n",
    "image = image.unsqueeze(0)\n",
    "\n",
    "\n",
    "brain.eval()\n",
    "with torch.no_grad():\n",
    "    pred_class, pred_box = brain(image)\n",
    "\n",
    "print(\"single pred: \", pred_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47cd6b3a-cc96-4e13-b247-11bf21fbe461",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "conclusion:\n",
    "\n",
    "the accuracy is garbage. images are way different than the training ones.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d43c05a9-3fc2-4948-9968-f56ccddc7c4a",
   "metadata": {},
   "source": [
    "## 8. testing2 (correctly?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c2e1a88e-fb20-4f56-afc2-2a6fee592ce5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class true \t prediction \t prediction prob \t bounding box\n",
      "1 \t\t 1 \t\t 0.999991 \t\t tensor([[0.1248, 0.3619, 0.8698, 0.6428]])\n",
      "1 \t\t 1 \t\t 0.999887 \t\t tensor([[0.1507, 0.4131, 0.8519, 0.5950]])\n",
      "0 \t\t 0 \t\t 0.000006 \t\t tensor([[0.1309, 0.3022, 0.8731, 0.7191]])\n",
      "1 \t\t 1 \t\t 0.999997 \t\t tensor([[0.1185, 0.3664, 0.8834, 0.6310]])\n",
      "1 \t\t 1 \t\t 0.999992 \t\t tensor([[0.1301, 0.3702, 0.8688, 0.6332]])\n",
      "1 \t\t 1 \t\t 0.999961 \t\t tensor([[0.1235, 0.3582, 0.8742, 0.6489]])\n",
      "0 \t\t 0 \t\t 0.000001 \t\t tensor([[0.1268, 0.2781, 0.8657, 0.7408]])\n",
      "0 \t\t 0 \t\t 0.000001 \t\t tensor([[0.1398, 0.2929, 0.8631, 0.7366]])\n",
      "1 \t\t 1 \t\t 0.999987 \t\t tensor([[0.1244, 0.4154, 0.8801, 0.5948]])\n",
      "1 \t\t 1 \t\t 0.999721 \t\t tensor([[0.1548, 0.3794, 0.8365, 0.6198]])\n",
      "1 \t\t 1 \t\t 0.999993 \t\t tensor([[0.1371, 0.3610, 0.8592, 0.6463]])\n",
      "0 \t\t 0 \t\t 0.000012 \t\t tensor([[0.1709, 0.3166, 0.8354, 0.7073]])\n",
      "1 \t\t 1 \t\t 0.999999 \t\t tensor([[0.1595, 0.4231, 0.8399, 0.6077]])\n",
      "0 \t\t 0 \t\t 0.000003 \t\t tensor([[0.1405, 0.2930, 0.8660, 0.7016]])\n",
      "0 \t\t 0 \t\t 0.000182 \t\t tensor([[0.1350, 0.2665, 0.8671, 0.7037]])\n",
      "0 \t\t 0 \t\t 0.000000 \t\t tensor([[0.1437, 0.3082, 0.8714, 0.7299]])\n",
      "0 \t\t 0 \t\t 0.000001 \t\t tensor([[0.1278, 0.2985, 0.8767, 0.6896]])\n",
      "1 \t\t 1 \t\t 0.999990 \t\t tensor([[0.1188, 0.3802, 0.8839, 0.6136]])\n",
      "1 \t\t 1 \t\t 1.000000 \t\t tensor([[0.1400, 0.3625, 0.8609, 0.6447]])\n",
      "0 \t\t 0 \t\t 0.000002 \t\t tensor([[0.1367, 0.2964, 0.8697, 0.7019]])\n",
      "1 \t\t 1 \t\t 0.999994 \t\t tensor([[0.1436, 0.3911, 0.8591, 0.6094]])\n",
      "0 \t\t 0 \t\t 0.000004 \t\t tensor([[0.1332, 0.2776, 0.8770, 0.7261]])\n",
      "1 \t\t 1 \t\t 0.999996 \t\t tensor([[0.1443, 0.3858, 0.8587, 0.6155]])\n",
      "0 \t\t 0 \t\t 0.000000 \t\t tensor([[0.1346, 0.2727, 0.8658, 0.7489]])\n",
      "0 \t\t 0 \t\t 0.000000 \t\t tensor([[0.1174, 0.2776, 0.8880, 0.7396]])\n",
      "1 \t\t 1 \t\t 0.999856 \t\t tensor([[0.1465, 0.3867, 0.8505, 0.6085]])\n",
      "0 \t\t 0 \t\t 0.000000 \t\t tensor([[0.1261, 0.2725, 0.8833, 0.7041]])\n",
      "1 \t\t 1 \t\t 0.999999 \t\t tensor([[0.1558, 0.3775, 0.8493, 0.6349]])\n",
      "0 \t\t 0 \t\t 0.000007 \t\t tensor([[0.1863, 0.3052, 0.8123, 0.6828]])\n",
      "1 \t\t 1 \t\t 0.999995 \t\t tensor([[0.1208, 0.3954, 0.8813, 0.5997]])\n",
      "0 \t\t 0 \t\t 0.000004 \t\t tensor([[0.1299, 0.2994, 0.8778, 0.6907]])\n",
      "1 \t\t 1 \t\t 0.999989 \t\t tensor([[0.1409, 0.3817, 0.8552, 0.6287]])\n",
      "1 \t\t 1 \t\t 0.999999 \t\t tensor([[0.1611, 0.4413, 0.8394, 0.6203]])\n",
      "1 \t\t 1 \t\t 0.999997 \t\t tensor([[0.1301, 0.4011, 0.8753, 0.6031]])\n",
      "0 \t\t 0 \t\t 0.000002 \t\t tensor([[0.1334, 0.3272, 0.8716, 0.7779]])\n",
      "1 \t\t 1 \t\t 0.999964 \t\t tensor([[0.1268, 0.3893, 0.8735, 0.6011]])\n",
      "0 \t\t 0 \t\t 0.000002 \t\t tensor([[0.1356, 0.2555, 0.8753, 0.7380]])\n",
      "0 \t\t 0 \t\t 0.000002 \t\t tensor([[0.1381, 0.2510, 0.8708, 0.7317]])\n",
      "1 \t\t 1 \t\t 0.999989 \t\t tensor([[0.1053, 0.3623, 0.8923, 0.6330]])\n",
      "0 \t\t 0 \t\t 0.000044 \t\t tensor([[0.1417, 0.3246, 0.8722, 0.7032]])\n",
      "0 \t\t 0 \t\t 0.000034 \t\t tensor([[0.1500, 0.3207, 0.8472, 0.6992]])\n",
      "0 \t\t 0 \t\t 0.000003 \t\t tensor([[0.1444, 0.2843, 0.8613, 0.6925]])\n",
      "1 \t\t 1 \t\t 0.999990 \t\t tensor([[0.1376, 0.4072, 0.8660, 0.6053]])\n",
      "0 \t\t 0 \t\t 0.000004 \t\t tensor([[0.1329, 0.2729, 0.8770, 0.7288]])\n",
      "0 \t\t 0 \t\t 0.000012 \t\t tensor([[0.1207, 0.2608, 0.8877, 0.7272]])\n",
      "0 \t\t 0 \t\t 0.000000 \t\t tensor([[0.1176, 0.2886, 0.8882, 0.7280]])\n",
      "0 \t\t 0 \t\t 0.000002 \t\t tensor([[0.1307, 0.2721, 0.8711, 0.7323]])\n",
      "0 \t\t 0 \t\t 0.000003 \t\t tensor([[0.1336, 0.2991, 0.8779, 0.7411]])\n",
      "0 \t\t 0 \t\t 0.000000 \t\t tensor([[0.1291, 0.2832, 0.8781, 0.6870]])\n",
      "0 \t\t 0 \t\t 0.000001 \t\t tensor([[0.1406, 0.2750, 0.8595, 0.7183]])\n",
      "0 \t\t 0 \t\t 0.000001 \t\t tensor([[0.1250, 0.2991, 0.8661, 0.7323]])\n",
      "0 \t\t 0 \t\t 0.000011 \t\t tensor([[0.1649, 0.2935, 0.8361, 0.7229]])\n",
      "0 \t\t 0 \t\t 0.000001 \t\t tensor([[0.1468, 0.2787, 0.8673, 0.7050]])\n",
      "0 \t\t 0 \t\t 0.000004 \t\t tensor([[0.1433, 0.2599, 0.8619, 0.7385]])\n",
      "0 \t\t 0 \t\t 0.000002 \t\t tensor([[0.1416, 0.2487, 0.8571, 0.7222]])\n",
      "0 \t\t 0 \t\t 0.000024 \t\t tensor([[0.1535, 0.2752, 0.8501, 0.7096]])\n",
      "0 \t\t 0 \t\t 0.000019 \t\t tensor([[0.1430, 0.2789, 0.8464, 0.7154]])\n",
      "0 \t\t 0 \t\t 0.000077 \t\t tensor([[0.1455, 0.3032, 0.8647, 0.6823]])\n",
      "1 \t\t 1 \t\t 0.999932 \t\t tensor([[0.1398, 0.3811, 0.8529, 0.6219]])\n",
      "0 \t\t 0 \t\t 0.000002 \t\t tensor([[0.1743, 0.3054, 0.8381, 0.6641]])\n",
      "0 \t\t 0 \t\t 0.000001 \t\t tensor([[0.1690, 0.3373, 0.8370, 0.7061]])\n",
      "0 \t\t 0 \t\t 0.000000 \t\t tensor([[0.1142, 0.2519, 0.8860, 0.7530]])\n",
      "0 \t\t 0 \t\t 0.000000 \t\t tensor([[0.1191, 0.2987, 0.8830, 0.7251]])\n",
      "0 \t\t 0 \t\t 0.000000 \t\t tensor([[0.1351, 0.2551, 0.8685, 0.7566]])\n",
      "1 \t\t 1 \t\t 0.999981 \t\t tensor([[0.1270, 0.4142, 0.8770, 0.5996]])\n",
      "0 \t\t 0 \t\t 0.000006 \t\t tensor([[0.1325, 0.2913, 0.8740, 0.7192]])\n",
      "0 \t\t 0 \t\t 0.000006 \t\t tensor([[0.1362, 0.3411, 0.8719, 0.7523]])\n",
      "0 \t\t 0 \t\t 0.000000 \t\t tensor([[0.1242, 0.2543, 0.8805, 0.7434]])\n",
      "1 \t\t 1 \t\t 0.999956 \t\t tensor([[0.1553, 0.4177, 0.8472, 0.5926]])\n",
      "0 \t\t 0 \t\t 0.000001 \t\t tensor([[0.1446, 0.2537, 0.8651, 0.7217]])\n",
      "1 \t\t 1 \t\t 0.999934 \t\t tensor([[0.1363, 0.4011, 0.8639, 0.5990]])\n",
      "0 \t\t 0 \t\t 0.000004 \t\t tensor([[0.1649, 0.3220, 0.8435, 0.7037]])\n",
      "0 \t\t 0 \t\t 0.000012 \t\t tensor([[0.1569, 0.3404, 0.8591, 0.7325]])\n",
      "0 \t\t 0 \t\t 0.000028 \t\t tensor([[0.1343, 0.2915, 0.8729, 0.7194]])\n",
      "0 \t\t 0 \t\t 0.000001 \t\t tensor([[0.1309, 0.3087, 0.8713, 0.7447]])\n",
      "1 \t\t 1 \t\t 0.999989 \t\t tensor([[0.1173, 0.3815, 0.8858, 0.6211]])\n",
      "1 \t\t 1 \t\t 0.999999 \t\t tensor([[0.1325, 0.3802, 0.8627, 0.6285]])\n",
      "1 \t\t 1 \t\t 0.999988 \t\t tensor([[0.1192, 0.3546, 0.8811, 0.6426]])\n",
      "0 \t\t 0 \t\t 0.000000 \t\t tensor([[0.1270, 0.2517, 0.8795, 0.7324]])\n",
      "1 \t\t 1 \t\t 1.000000 \t\t tensor([[0.1243, 0.3676, 0.8721, 0.6314]])\n",
      "0 \t\t 0 \t\t 0.000000 \t\t tensor([[0.1307, 0.2905, 0.8698, 0.7042]])\n",
      "0 \t\t 0 \t\t 0.000001 \t\t tensor([[0.1150, 0.2377, 0.8870, 0.7417]])\n",
      "0 \t\t 0 \t\t 0.000001 \t\t tensor([[0.1492, 0.2748, 0.8521, 0.7064]])\n",
      "0 \t\t 0 \t\t 0.000063 \t\t tensor([[0.1526, 0.2965, 0.8448, 0.7156]])\n",
      "1 \t\t 1 \t\t 0.999999 \t\t tensor([[0.1283, 0.3727, 0.8747, 0.6226]])\n",
      "1 \t\t 1 \t\t 0.999999 \t\t tensor([[0.1393, 0.3525, 0.8648, 0.6332]])\n",
      "0 \t\t 0 \t\t 0.000002 \t\t tensor([[0.1294, 0.2567, 0.8711, 0.7176]])\n",
      "0 \t\t 0 \t\t 0.000004 \t\t tensor([[0.1162, 0.2703, 0.8828, 0.6972]])\n",
      "1 \t\t 1 \t\t 0.999996 \t\t tensor([[0.1289, 0.3810, 0.8724, 0.6243]])\n",
      "1 \t\t 1 \t\t 1.000000 \t\t tensor([[0.1489, 0.3663, 0.8557, 0.6328]])\n",
      "1 \t\t 1 \t\t 0.999984 \t\t tensor([[0.1329, 0.3774, 0.8698, 0.6206]])\n",
      "0 \t\t 0 \t\t 0.000096 \t\t tensor([[0.1519, 0.3205, 0.8508, 0.7030]])\n",
      "0 \t\t 0 \t\t 0.000001 \t\t tensor([[0.1297, 0.2496, 0.8714, 0.7306]])\n",
      "0 \t\t 1 \t\t 0.999992 \t\t tensor([[0.1217, 0.3735, 0.8750, 0.6264]])\n",
      "0 \t\t 0 \t\t 0.000014 \t\t tensor([[0.1453, 0.2732, 0.8551, 0.7328]])\n",
      "0 \t\t 0 \t\t 0.000018 \t\t tensor([[0.1435, 0.2672, 0.8606, 0.7100]])\n",
      "1 \t\t 1 \t\t 0.999998 \t\t tensor([[0.1095, 0.3368, 0.8858, 0.6375]])\n",
      "0 \t\t 0 \t\t 0.000001 \t\t tensor([[0.1290, 0.2552, 0.8687, 0.7313]])\n",
      "1 \t\t 1 \t\t 0.999999 \t\t tensor([[0.1355, 0.3369, 0.8670, 0.6530]])\n",
      "1 \t\t 1 \t\t 1.000000 \t\t tensor([[0.1252, 0.3460, 0.8733, 0.6511]])\n",
      "1 \t\t 1 \t\t 0.999993 \t\t tensor([[0.1275, 0.4154, 0.8714, 0.5998]])\n",
      "1 \t\t 1 \t\t 0.999941 \t\t tensor([[0.1407, 0.3747, 0.8545, 0.6355]])\n",
      "1 \t\t 1 \t\t 0.999997 \t\t tensor([[0.1223, 0.3877, 0.8791, 0.6073]])\n",
      "1 \t\t 1 \t\t 0.999983 \t\t tensor([[0.1197, 0.3886, 0.8808, 0.6171]])\n",
      "1 \t\t 1 \t\t 0.999981 \t\t tensor([[0.1310, 0.3975, 0.8670, 0.6066]])\n",
      "1 \t\t 1 \t\t 0.999997 \t\t tensor([[0.1336, 0.3778, 0.8676, 0.6186]])\n",
      "1 \t\t 1 \t\t 0.999999 \t\t tensor([[0.1293, 0.3415, 0.8762, 0.6422]])\n",
      "1 \t\t 1 \t\t 0.999995 \t\t tensor([[0.1389, 0.4049, 0.8612, 0.5991]])\n",
      "1 \t\t 1 \t\t 0.999998 \t\t tensor([[0.1339, 0.3967, 0.8695, 0.6048]])\n",
      "1 \t\t 1 \t\t 0.999995 \t\t tensor([[0.1215, 0.3227, 0.8784, 0.6562]])\n",
      "1 \t\t 1 \t\t 0.999996 \t\t tensor([[0.1218, 0.3933, 0.8776, 0.6103]])\n",
      "1 \t\t 1 \t\t 0.999962 \t\t tensor([[0.1465, 0.3663, 0.8518, 0.6332]])\n",
      "1 \t\t 1 \t\t 0.999991 \t\t tensor([[0.1289, 0.3630, 0.8762, 0.6582]])\n",
      "0 \t\t 0 \t\t 0.000010 \t\t tensor([[0.1581, 0.2665, 0.8417, 0.7118]])\n",
      "1 \t\t 1 \t\t 0.999998 \t\t tensor([[0.1384, 0.3929, 0.8634, 0.6126]])\n",
      "1 \t\t 1 \t\t 0.999762 \t\t tensor([[0.1622, 0.4088, 0.8367, 0.6043]])\n",
      "1 \t\t 1 \t\t 0.999983 \t\t tensor([[0.1441, 0.3959, 0.8577, 0.6064]])\n",
      "0 \t\t 0 \t\t 0.000010 \t\t tensor([[0.1589, 0.2922, 0.8379, 0.7282]])\n",
      "1 \t\t 1 \t\t 0.999956 \t\t tensor([[0.1326, 0.3470, 0.8627, 0.6312]])\n",
      "1 \t\t 1 \t\t 0.999985 \t\t tensor([[0.1220, 0.3882, 0.8760, 0.6071]])\n",
      "1 \t\t 1 \t\t 1.000000 \t\t tensor([[0.1225, 0.3195, 0.8791, 0.6521]])\n",
      "1 \t\t 1 \t\t 0.999919 \t\t tensor([[0.1402, 0.3796, 0.8578, 0.6168]])\n",
      "1 \t\t 1 \t\t 0.999967 \t\t tensor([[0.1252, 0.3976, 0.8724, 0.6102]])\n",
      "1 \t\t 1 \t\t 0.999876 \t\t tensor([[0.1251, 0.3224, 0.8681, 0.6808]])\n",
      "1 \t\t 1 \t\t 0.999999 \t\t tensor([[0.1384, 0.3825, 0.8613, 0.6158]])\n",
      "1 \t\t 1 \t\t 0.999992 \t\t tensor([[0.1276, 0.4103, 0.8739, 0.6007]])\n",
      "1 \t\t 1 \t\t 0.999998 \t\t tensor([[0.1257, 0.3365, 0.8673, 0.6605]])\n",
      "1 \t\t 1 \t\t 0.999993 \t\t tensor([[0.1345, 0.3761, 0.8642, 0.6209]])\n",
      "0 \t\t 0 \t\t 0.000000 \t\t tensor([[0.1446, 0.2749, 0.8605, 0.7148]])\n",
      "0 \t\t 0 \t\t 0.000003 \t\t tensor([[0.1524, 0.2932, 0.8549, 0.7093]])\n",
      "1 \t\t 1 \t\t 0.999972 \t\t tensor([[0.1683, 0.4328, 0.8240, 0.6040]])\n",
      "0 \t\t 0 \t\t 0.000002 \t\t tensor([[0.1601, 0.2984, 0.8511, 0.7002]])\n",
      "1 \t\t 1 \t\t 0.999999 \t\t tensor([[0.1081, 0.3474, 0.8869, 0.6492]])\n",
      "0 \t\t 0 \t\t 0.000001 \t\t tensor([[0.1382, 0.2729, 0.8684, 0.7118]])\n",
      "1 \t\t 1 \t\t 0.999961 \t\t tensor([[0.1352, 0.3802, 0.8623, 0.6169]])\n",
      "1 \t\t 1 \t\t 0.999996 \t\t tensor([[0.1314, 0.3977, 0.8726, 0.6120]])\n",
      "0 \t\t 0 \t\t 0.000001 \t\t tensor([[0.1417, 0.3027, 0.8669, 0.6981]])\n",
      "1 \t\t 1 \t\t 0.999952 \t\t tensor([[0.1215, 0.3879, 0.8792, 0.6100]])\n",
      "1 \t\t 1 \t\t 0.999958 \t\t tensor([[0.1447, 0.3845, 0.8489, 0.6153]])\n",
      "1 \t\t 1 \t\t 0.999848 \t\t tensor([[0.1468, 0.3921, 0.8498, 0.6275]])\n",
      "1 \t\t 1 \t\t 0.999946 \t\t tensor([[0.1142, 0.3859, 0.8852, 0.6248]])\n",
      "1 \t\t 1 \t\t 0.999972 \t\t tensor([[0.1485, 0.3887, 0.8499, 0.6151]])\n",
      "1 \t\t 1 \t\t 1.000000 \t\t tensor([[0.1300, 0.3744, 0.8724, 0.6247]])\n",
      "0 \t\t 0 \t\t 0.000009 \t\t tensor([[0.1458, 0.2707, 0.8558, 0.7209]])\n",
      "0 \t\t 0 \t\t 0.000002 \t\t tensor([[0.1472, 0.2748, 0.8635, 0.7076]])\n",
      "1 \t\t 1 \t\t 0.999996 \t\t tensor([[0.1290, 0.3759, 0.8740, 0.6185]])\n",
      "0 \t\t 0 \t\t 0.000002 \t\t tensor([[0.1586, 0.2963, 0.8518, 0.6784]])\n",
      "1 \t\t 1 \t\t 0.999950 \t\t tensor([[0.1322, 0.3818, 0.8686, 0.6079]])\n",
      "1 \t\t 1 \t\t 0.999954 \t\t tensor([[0.1443, 0.3780, 0.8525, 0.6266]])\n",
      "0 \t\t 0 \t\t 0.000000 \t\t tensor([[0.1430, 0.3004, 0.8665, 0.7174]])\n",
      "1 \t\t 1 \t\t 0.999991 \t\t tensor([[0.1319, 0.3830, 0.8706, 0.6174]])\n",
      "0 \t\t 0 \t\t 0.000007 \t\t tensor([[0.1450, 0.3169, 0.8656, 0.6766]])\n",
      "1 \t\t 1 \t\t 0.998976 \t\t tensor([[0.1654, 0.4061, 0.8277, 0.6001]])\n",
      "1 \t\t 1 \t\t 1.000000 \t\t tensor([[0.1269, 0.3634, 0.8657, 0.6278]])\n",
      "1 \t\t 1 \t\t 0.999985 \t\t tensor([[0.1208, 0.3607, 0.8784, 0.6334]])\n",
      "1 \t\t 1 \t\t 0.999995 \t\t tensor([[0.1392, 0.3686, 0.8502, 0.6258]])\n",
      "1 \t\t 1 \t\t 0.999959 \t\t tensor([[0.1442, 0.3780, 0.8472, 0.6274]])\n",
      "0 \t\t 0 \t\t 0.000007 \t\t tensor([[0.1202, 0.2778, 0.8899, 0.7122]])\n",
      "0 \t\t 0 \t\t 0.000080 \t\t tensor([[0.1466, 0.3031, 0.8553, 0.7268]])\n"
     ]
    }
   ],
   "source": [
    "# testing\n",
    "\n",
    "test_loader = DataLoader(test_set, batch_size=1, shuffle=False)\n",
    "predictions = []\n",
    "\n",
    "brain.eval()\n",
    "with torch.no_grad():\n",
    "    for img, label, annotation in test_loader:\n",
    "        pred_class, pred_box = brain(img)\n",
    "        predictions.append({\"class true\" : int(label),\n",
    "                            \"prediction prob\" : float(pred_class),\n",
    "                            \"bounding_box\" : pred_box})\n",
    "\n",
    "\n",
    "print(\"class true \\t prediction \\t prediction prob \\t bounding box\")\n",
    "for elem in predictions:\n",
    "    \"\"\"\n",
    "    if elem[\"class true\"] == 1:\n",
    "        elem[\"class true\"] = \"airplane\"\n",
    "    else:\n",
    "        elem[\"class true\"] = \"motorbike\"\n",
    "    \"\"\"\n",
    "    if elem[\"prediction prob\"] >= 0.5:\n",
    "        elem[\"prediction\"] = 1\n",
    "    else:\n",
    "        elem[\"prediction\"] = 0\n",
    "    if elem[\"prediction prob\"]:\n",
    "        elem[\"prediction prob\"] = \"{:.6f}\".format(elem[\"prediction prob\"])\n",
    "    print(elem[\"class true\"], \"\\t\\t\", elem[\"prediction\"], \"\\t\\t\", elem[\"prediction prob\"], \"\\t\\t\", elem[\"bounding_box\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9e91673c-b3bc-4cde-95f3-6545000515d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sklearn accuracy:  0.9937106918238994\n",
      "np accuracy:  0.9937106918238994\n"
     ]
    }
   ],
   "source": [
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "for elem in predictions:\n",
    "    y_true.append(elem[\"class true\"])\n",
    "    y_pred.append(elem[\"prediction\"])\n",
    "print(\"sklearn accuracy: \", accuracy_score(y_true, y_pred))\n",
    "\n",
    "np_acc = np.sum(np.equal(y_true, y_pred)) / len(y_true)\n",
    "print(\"np accuracy: \", np_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ffd5cd6f-e667-402f-b855-1b6f4d56b061",
   "metadata": {},
   "outputs": [],
   "source": [
    "# too high accuracy. -> overfitting?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "85e10db3-4e7e-4b1e-bb8a-03bfc74eb5a7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rectangle true:  [40, 34, 346, 138]\n",
      "rectangle pred:  [50.055305, 28.773949, 347.00525, 141.4047]\n",
      "class pred:  1.0000\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rectangle true:  [52, 27, 348, 96]\n",
      "rectangle pred:  [59.52725, 25.624315, 336.505, 97.47928]\n",
      "class pred:  0.9999\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rectangle true:  [29, 25, 229, 136]\n",
      "rectangle pred:  [34.157013, 27.611288, 227.88611, 135.24326]\n",
      "class pred:  0.0000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[33], line 41\u001b[0m\n\u001b[0;32m     39\u001b[0m img_orig\u001b[38;5;241m.\u001b[39mshow()\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# wait for buttonpress\u001b[39;00m\n\u001b[1;32m---> 41\u001b[0m \u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mE:\\programozas\\machine_learning\\udemy_machinelearn+old_tensorflow_basics\\.venv\\lib\\site-packages\\ipykernel\\kernelbase.py:1282\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[1;34m(self, prompt)\u001b[0m\n\u001b[0;32m   1280\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1281\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[1;32m-> 1282\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_input_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1283\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1284\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parent_ident\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1285\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_parent\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1286\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1287\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mE:\\programozas\\machine_learning\\udemy_machinelearn+old_tensorflow_basics\\.venv\\lib\\site-packages\\ipykernel\\kernelbase.py:1325\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[1;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[0;32m   1322\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[0;32m   1323\u001b[0m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[0;32m   1324\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1325\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1326\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1327\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "# check bounding box\n",
    "\n",
    "    \n",
    "for img_path, annotation_path in zip(test_set.image_paths, test_set.annotation_paths):\n",
    "    #raise Exception(\"bounding box might be good, but need to figure out the correct rectangle drawing. see above at preprocessing\")\n",
    "    # use cv2 to draw the rectangle\n",
    "    #cv_img = cv2.imread(img_path)\n",
    "    \n",
    "    img_orig = PIL.Image.open(img_path)\n",
    "    img = PIL.Image.open(img_path).convert(\"RGB\")\n",
    "    annot = scipy.io.loadmat(annotation_path)\n",
    "    annot = annot[\"box_coord\"].squeeze()\n",
    "    print(\"rectangle true: \", [annot[2], annot[0], annot[3], annot[1]])\n",
    "\n",
    "    img, scale, padding = preprocess_image(img)\n",
    "    padded_img_width, padded_img_height = 224, 224\n",
    "    annot = preprocess_bounding_box(annot, img, scale, padding)\n",
    "\n",
    "    # predict\n",
    "    brain.eval()\n",
    "    with torch.no_grad():\n",
    "        pred_class, pred_box = brain(img.unsqueeze(0))\n",
    "\n",
    "    pred_box = pred_box.squeeze()\n",
    "    reverted_bbox = preprocess_bounding_box(pred_box, img, scale, padding, reverse=True)\n",
    "    x1, y1, x2, y2 = reverted_bbox.numpy()\n",
    "    #print(pred_box)\n",
    "\n",
    "    \"\"\"\n",
    "    ! should reconvert the img and bbox to see as if it worked on original img.\n",
    "    the user in practical wanna see the real result, not the result on preprocessed img.\n",
    "    -> make a \"reverse=False\" argument on the preprocess bbox function. (for img we should use the original, instead of reconvert)\n",
    "    \"\"\"\n",
    "    # draw rectangle on img\n",
    "    print(\"rectangle pred: \", [x1, y1, x2, y2])\n",
    "    print(\"class pred: \", f\"{float(pred_class):.4f}\")\n",
    "    draw_obj = PIL.ImageDraw.Draw(img_orig)\n",
    "    draw_obj.rectangle([x1, y1, x2, y2], outline=\"red\")\n",
    "    img_orig.show()\n",
    "    # wait for buttonpress\n",
    "    input(\"\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b40899-8650-4481-8754-7c1786028fc6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
