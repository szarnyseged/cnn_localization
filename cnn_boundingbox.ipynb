{
 "cells": [
  {
   "cell_type": "raw",
   "id": "ff1f750f-3868-437a-8a1a-eb8a366703fe",
   "metadata": {},
   "source": [
    "#\n",
    "bounding box regression on caltech 101 dataset.\n",
    "limited to binary classification\n",
    "dataset source: https://data.caltech.edu/records/mzrjq-6wc02\n",
    "\n",
    "dataset description:\n",
    "Pictures of objects belonging to 101 categories. About 40 to 800 images per category. Most categories have about 50 images. Collected in September 2003 by Fei-Fei Li, Marco Andreetto, and Marc'Aurelio Ranzato. The size of each image is roughly 300 x 200 pixels. We have carefully clicked outlines of each object in these pictures, these are included under the 'Annotations.tar'. There is also a MATLAB script to view the annotations, 'show_annotations.m'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad759170-945a-40bf-bfa4-e1ae16c6f854",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader, Subset, Dataset\n",
    "from torchvision.datasets import DatasetFolder, ImageFolder\n",
    "from torchvision import transforms\n",
    "import random\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "709bc3f1-3e69-499d-931f-7c494c75dce3",
   "metadata": {},
   "source": [
    "# 1. preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3383bdb0-28ce-4e4d-bbcd-9b86782a08ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nplan:\\nimages and box labels are in separate files. -> we have to connect them -> create a dataset for dataloading\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "plan:\n",
    "images and box labels are in separate files. -> we have to connect them -> create a dataset for dataloading\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ceb03e57-b021-407c-8c2e-35e33b8269c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "import PIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cacaab0e-2ad5-4be1-b50e-37ea80326c82",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'__header__': b'MATLAB 5.0 MAT-file, Platform: PCWIN, Created on: Tue Dec 14 11:03:29 2004', '__version__': '1.0', '__globals__': [], 'box_coord': array([[ 30, 137,  49, 349]], dtype=uint16), 'obj_contour': array([[  8.54082661,  11.87852823,   1.86542339,   1.56199597,\n",
      "         31.60131048,  27.65675403,  23.71219758,  18.85735887,\n",
      "         18.85735887,  31.60131048,  47.68296371,  51.32409274,\n",
      "         59.51663306,  60.1234879 ,  56.78578629,  78.02570565,\n",
      "         91.07308468, 178.46018145, 179.97731855, 222.15372984,\n",
      "        225.79485887, 239.75252016, 265.84727823, 298.92086694,\n",
      "        300.13457661, 298.3140121 , 265.54385081, 264.63356855,\n",
      "        270.39868952, 268.88155242, 265.84727823, 264.02671371,\n",
      "        260.08215726, 255.83417339, 257.6547379 , 261.90272177,\n",
      "        261.90272177, 160.25453629, 160.25453629, 156.00655242,\n",
      "        155.39969758, 149.33114919, 142.04889113, 139.31804435,\n",
      "        139.92489919, 143.26260081, 136.28377016, 128.09122984,\n",
      "        124.45010081, 123.53981855, 122.93296371, 117.47127016,\n",
      "         73.77772177,  68.61945565,  44.95211694,  38.27671371,\n",
      "         36.15272177,   8.23739919,   8.54082661],\n",
      "       [ 75.89163306,  62.54082661,  59.80997984,  56.77570565,\n",
      "         51.01058468,  35.23235887,  11.26159274,  10.35131048,\n",
      "          6.10332661,   1.2484879 ,   3.37247984,  12.171875  ,\n",
      "         12.47530242,  19.15070565,  18.54385081,  47.97631048,\n",
      "         51.3140121 ,  49.19002016,  44.03175403,  45.54889113,\n",
      "         51.3140121 ,  55.25856855,  72.85735887,  85.60131048,\n",
      "         90.15272177,  90.75957661,  92.58014113,  94.09727823,\n",
      "         95.3109879 , 100.77268145, 101.37953629, 104.7172379 ,\n",
      "        105.93094758, 102.28981855,  95.3109879 ,  94.70413306,\n",
      "         91.66985887,  90.15272177,  97.13155242,  97.73840726,\n",
      "        102.89667339, 105.32409274, 104.7172379 ,  97.43497984,\n",
      "         93.79385081,  91.36643145,  88.9390121 ,  88.33215726,\n",
      "         92.88356855,  92.58014113,  88.63558468,  86.81502016,\n",
      "         83.78074597,  85.9047379 ,  82.87046371,  85.60131048,\n",
      "         80.13961694,  75.28477823,  75.58820565]])}\n"
     ]
    }
   ],
   "source": [
    "# insights on matfile\n",
    "\n",
    "mat_file = scipy.io.loadmat(\"./caltech-101/Annotations/Airplanes_Side_2/annotation_0001.mat\")\n",
    "print(mat_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1579aaed-3fd1-4046-91ad-c498c0e7723c",
   "metadata": {},
   "source": [
    "## 2. build the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "124fa595-cb32-42b3-8d2a-8ac9487fd184",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "\n",
    "    def __init__(self, n_classes, seed=101):\n",
    "        super().__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, stride=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            nn.Conv2d(32, 32, kernel_size=3, stride=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            nn.Conv2d(32, 32, kernel_size=3, stride=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "\n",
    "        self.n_flattened_neurons = 10304\n",
    "        \n",
    "        # classification head\n",
    "        self.class_head = nn.Sequential(\n",
    "            nn.Linear(self.n_flattened_neurons, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, n_classes),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        # localization head\n",
    "        self.localize_head = nn.Sequential(\n",
    "            nn.Linear(self.n_flattened_neurons, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 4),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, data):\n",
    "        signal = self.features(data)\n",
    "        signal = self.flatten(signal)\n",
    "        #print(signal.size(1))\n",
    "        classes = self.class_head(signal)\n",
    "        bounding_boxes = self.localize_head(signal)\n",
    "        return classes, bounding_boxes"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7eb5f6fa-eedc-4454-b193-2edba7beb051",
   "metadata": {},
   "source": [
    "# test\n",
    "\n",
    "testwork = Network(1)\n",
    "test_img = PIL.Image.open(\"./caltech-101/subset/images/airplanes/image_0001.jpg\")\n",
    "transformer = transforms.Compose([transforms.Resize((200, 133)), transforms.ToTensor()])\n",
    "print(testwork(transformer(test_img).unsqueeze(0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9cb346-7673-427a-8901-28ea48b69183",
   "metadata": {},
   "source": [
    "## 3. preprocess dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8197cef6-02cd-4f14-a2a1-c9dbbb5a41f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 28 116  71 326]\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=405x140 at 0x1FDD9CFD0F0>\n",
      "pil:  405 140\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nrectangle looks like: y1, y2, x1, x2 ->  upper left and bottom right corner\\n\\n  x--------\\n |        |\\n |        |\\n  --------x\\n  \\n'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# insights on image and box\n",
    "\n",
    "mat_file = scipy.io.loadmat(\"./caltech-101/Annotations/Airplanes_Side_2/annotation_0056.mat\")\n",
    "coords = mat_file[\"box_coord\"].squeeze()\n",
    "print(coords)\n",
    "\n",
    "# !! cv returns (h, w) <-> pil returns (w, h)\n",
    "image = PIL.Image.open(\"./caltech-101/subset/images/airplanes/image_0056.jpg\")\n",
    "print(image)\n",
    "w, h = image.size\n",
    "print(\"pil: \", w, h)\n",
    "#image.show()\n",
    "\n",
    "draw_obj = PIL.ImageDraw.Draw(image)\n",
    "draw_obj.rectangle([coords[2], coords[0], coords[3], coords[1]], outline=\"red\")\n",
    "image.show()\n",
    "\n",
    "\"\"\"\n",
    "rectangle looks like: y1, y2, x1, x2 ->  upper left and bottom right corner\n",
    "\n",
    "  x--------\n",
    " |        |\n",
    " |        |\n",
    "  --------x\n",
    "  \n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "330523fd-95ca-45ca-ac9a-34b06ee38eba",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# !! image preprocess with padding respecting aspect ratio solution:\n",
    "\n",
    "# Function to resize with padding\n",
    "# The size of each image is roughly 300 x 200 pixels. (according to dataset description) \n",
    "def resize_with_padding(image, target_size=224, return_scale_and_pad=True):\n",
    "    \"\"\"\n",
    "    Resize while keeping aspect ratio and adding padding to match target_size.\n",
    "    return_scale_and_pad: useful info for bboxes, if they have to be corrected.\n",
    "    returns: (image, scale, tuple(padding))\n",
    "    \"\"\"\n",
    "    w, h = image.size\n",
    "    \n",
    "    # Compute the new size while keeping the aspect ratio\n",
    "    scale = target_size / max(w, h)  # Scaling factor\n",
    "    new_w, new_h = int(w * scale), int(h * scale)\n",
    "    \n",
    "    \n",
    "    # Resize while preserving aspect ratio\n",
    "    \"\"\"\n",
    "    when resizing an image, new pixel values need to be estimated.\n",
    "    which interpolation to use?\n",
    "    InterpolationMode.NEAREST → Fastest, but blocky (uses the nearest pixel).\n",
    "    InterpolationMode.BILINEAR → Smooth results, good for most cases.\n",
    "    InterpolationMode.BICUBIC → Even smoother, but slightly slower.\n",
    "    InterpolationMode.LANCZOS → High-quality, best for large upscaling.\n",
    "    \"\"\"\n",
    "    resize_transform = transforms.Resize((new_h, new_w), interpolation=transforms.InterpolationMode.BILINEAR)\n",
    "    image = resize_transform(image)\n",
    "\n",
    "    # Compute padding values (left, top, right, bottom)\n",
    "    pad_w = target_size - new_w\n",
    "    pad_h = target_size - new_h\n",
    "    padding = (pad_w // 2, pad_h // 2, pad_w - pad_w // 2, pad_h - pad_h // 2)\n",
    "\n",
    "    # Apply padding + transform to tensor\n",
    "    # transforms.Pad expects left,top,right,bottom IN ORDER\n",
    "    pad_transform = transforms.Compose([transforms.Pad(padding, fill=255), transforms.ToTensor()])\n",
    "    image = pad_transform(image)\n",
    "\n",
    "    if return_scale_and_pad == True:\n",
    "        return image, scale, padding\n",
    "    else:\n",
    "        return image\n",
    "\n",
    "\n",
    "# the bounding box in the matfile correspond to -> upper left and bottom right corner\n",
    "# like: y1, y2, x1, x2\n",
    "# see above the insights\n",
    "def preprocess_bounding_box(coords, padded_img, scaling_factor, padding, reverse=False):\n",
    "    # padded img coming as a tensor -> must retransform\n",
    "    transformer = transforms.ToPILImage()\n",
    "    reconverted_img = transformer(padded_img)\n",
    "    padded_img = reconverted_img\n",
    "    padded_width, padded_height = padded_img.size\n",
    "    pad_w_left, pad_h_top, pad_w_right, pad_h_bottom = padding      \n",
    "    \n",
    "    if reverse == False:\n",
    "        y1, y2, x1, x2 = coords\n",
    "        # scale the original coords to get the new coords\n",
    "        x1 = x1 * scaling_factor\n",
    "        x2 = x2 * scaling_factor\n",
    "        y1 = y1 * scaling_factor\n",
    "        y2 = y2 * scaling_factor\n",
    "    \n",
    "        # compute new coords with padding + normalize (0 to 1)\n",
    "        x1 = (x1 + pad_w_left) / padded_width\n",
    "        x2 = (x2 + pad_w_right) / padded_width\n",
    "        y1 = (y1 + pad_h_top) / padded_height\n",
    "        y2 = (y2 + pad_h_bottom) / padded_height\n",
    "    \n",
    "    else:\n",
    "        # ! coords order is different\n",
    "        x1, y1, x2, y2 = coords\n",
    "        x1 = x1 * padded_width\n",
    "        x2 = x2 * padded_width\n",
    "        y1 = y1 * padded_height\n",
    "        y2 = y2 * padded_height\n",
    "\n",
    "        x1 = x1 - pad_w_left\n",
    "        x2 = x2 - pad_w_right\n",
    "        y1 = y1 - pad_h_top \n",
    "        y2 = y2 - pad_h_bottom\n",
    "\n",
    "        x1 = x1 / scaling_factor\n",
    "        x2 = x2 / scaling_factor\n",
    "        y1 = y1 / scaling_factor\n",
    "        y2 = y2 / scaling_factor\n",
    "\n",
    "    # reorder to draw rectangle later\n",
    "    coords = np.array((x1, y1, x2, y2))\n",
    "    coords = torch.from_numpy(coords).float()\n",
    "    \n",
    "    return coords\n",
    "\n",
    "# Load the image and the bbox coords\n",
    "image_path = \"./caltech-101/subset/images/airplanes/image_0032.jpg\"\n",
    "image = PIL.Image.open(image_path)\n",
    "mat_file = scipy.io.loadmat(\"./caltech-101/subset/annotations/airplanes/annotation_0032.mat\")\n",
    "coords = mat_file[\"box_coord\"].squeeze()\n",
    "\n",
    "# Apply the transformation\n",
    "padded_image = resize_with_padding(image)\n",
    "boxes = preprocess_bounding_box(coords, padded_image[0], padded_image[1], padded_image[2])\n",
    "\n",
    "# Reconvert to see the test\n",
    "transformer = transforms.ToPILImage()\n",
    "reconverted_img = transformer(padded_image[0])\n",
    "reconverted_width, reconverted_height = reconverted_img.size\n",
    "boxes = boxes.numpy()\n",
    "print(boxes)\n",
    "\n",
    "# Revert coord normalization\n",
    "x1 = boxes[0] * reconverted_width\n",
    "y1 = boxes[1] * reconverted_height\n",
    "x2 = boxes[2] * reconverted_width\n",
    "y2 = boxes[3] * reconverted_height\n",
    "\n",
    "print(x1, y1, x2, y2)\n",
    "\n",
    "draw_obj = PIL.ImageDraw.Draw(reconverted_img)\n",
    "draw_obj.rectangle([x1, y1, x2, y2], outline=\"red\")\n",
    "reconverted_img.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c319e837-476e-4580-896f-235e3579dd22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# image preprocessing\n",
    "\n",
    "\"\"\"\n",
    "gpt\n",
    "different dimension problem (aspect ratio): losing the aspect ratio, distorting the image can lead potentially to lose important details.\n",
    "\n",
    "techniques:\n",
    "Aspect Ratio Preservation:\n",
    "    When resizing, it's important to maintain the original aspect ratio to prevent distortion.\n",
    "Region of Interest (ROI) Cropping:\n",
    "    For multi-class problems where objects have varying sizes, you might consider identifying and cropping regions of interest (ROIs)\n",
    "    containing the objects before resizing.\n",
    "    This helps focus on relevant parts of the image and reduces the impact of resizing on unrelated areas.\n",
    "Letterboxing or Padding:\n",
    "    Instead of distorting the image, you can pad the image to fit a desired size without altering its aspect ratio.\n",
    "    This involves adding extra pixels around the image to achieve the desired dimensions.\n",
    "Data Augmentation:\n",
    "    Augmentation techniques such as random cropping, rotation, \n",
    "    and scaling during training can help the model become more robust to variations in object sizes and orientations.\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "if we keep the aspect ratio, the images might have different sizes in pixel.\n",
    "but the first fully connected layer (after flattening) should be initialized with a fixed number of neurons.\n",
    "the number of outputs after flattening will be different.\n",
    "\n",
    "solutions:\n",
    "Resize Images to a Common Size (aspect ratio + with padding?)\n",
    "\n",
    "Adaptive Pooling:\n",
    "    Instead of using fully connected layers, you can replace them with adaptive pooling layers\n",
    "    (e.g., nn.AdaptiveAvgPool2d or nn.AdaptiveMaxPool2d). \n",
    "    Adaptive pooling allows you to specify the output size, and it dynamically adjusts to different input sizes. \n",
    "    This way, the network can handle inputs of varying dimensions.\n",
    "you can use a combination of Resize and CenterCrop to resize the image while preserving its aspect ratio.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "! we also have to rescale the bounding box coordinates with respect to aspect ratio, + normalize them (to range[0,1])\n",
    "\n",
    "\n",
    "# Assuming (x1, y1) and (x2, y2) are the original bounding box coordinates\n",
    "original_width, original_height = original_image_size\n",
    "resized_width, resized_height = resized_image_size\n",
    "\n",
    "# Calculate scaling factors\n",
    "width_scale = resized_width / original_width\n",
    "height_scale = resized_height / original_height\n",
    "\n",
    "# Adjust bounding box coordinates\n",
    "new_x1 = int(x1 * width_scale)\n",
    "new_y1 = int(y1 * height_scale)\n",
    "new_x2 = int(x2 * width_scale)\n",
    "new_y2 = int(y2 * height_scale)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# The size of each image is roughly 300 x 200 pixels. (according to dataset description)\n",
    "# /1,5 -> target 200x133 to respect the aspect ratio\n",
    "\n",
    "\n",
    "def preprocess_img(image):\n",
    "    # ! transforms.Resize(Height, Width) -> reverse \n",
    "    transformer = transforms.Compose([transforms.Resize((133, 200)), transforms.ToTensor()])\n",
    "    return transformer(image)\n",
    "\n",
    "\n",
    "# the bounding box in the matfile correspond to -> upper left and bottom right corner\n",
    "# like: y1, y2, x1, x2\n",
    "# see above the insights\n",
    "def preprocess_bounding_box(coords, image):\n",
    "    y1, y2, x1, x2 = coords\n",
    "    width, height = image.size\n",
    "    x1 = x1 / width\n",
    "    y1 = y1 / height\n",
    "    x2 = x2 / width\n",
    "    y2 = y2 / height\n",
    "    # reorder to draw rectangle later\n",
    "    coords = np.array((x1, y1, x2, y2))\n",
    "    coords = torch.from_numpy(coords).float()\n",
    "\n",
    "    \n",
    "    return coords\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d25a2e2a-3b0d-42ee-9660-07349050744b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !!!! issue: now we ignored the aspect ratio of images.\n",
    "# -> works here, but problem with multi class with different aspect ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0867fe80-80d5-43d4-8ed6-351e3a6843b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ce067f-7ee6-4452-b1dd-5dfad26fb70b",
   "metadata": {},
   "source": [
    "## 4. init hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c2dff169-ecb2-4740-ae7c-d9f4be810df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "#learning_rate = 0.001\n",
    "learning_rate = 0.0001\n",
    "n_epochs = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c1f8fc-741a-43f1-9dd5-2bdd5efa824c",
   "metadata": {},
   "source": [
    "## 5. create dataset + dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ec696b61-d88c-458e-9bfc-6fedcc1625bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create custom dataset loader to connect the image + label + bounding box coords\n",
    "import warnings\n",
    "\n",
    "\n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    root_folder: relative path\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, root_folder=None, transform_img=None, transform_annot=None):\n",
    "\n",
    "        self.root_folder = root_folder\n",
    "        self.transform_img = transform_img\n",
    "        self.transform_annot = transform_annot\n",
    "        self.image_paths, self.class_label, self.annotation_paths = self.__get_paths_and_classes()\n",
    "\n",
    "\n",
    "    def __get_paths_and_classes(self):\n",
    "        if self.root_folder:\n",
    "            print(\"CustomDataset initializing...\")\n",
    "            image_paths = []\n",
    "            annotation_paths = []\n",
    "            classes_to_label = []\n",
    "    \n",
    "    \n",
    "            images_folder = os.path.join(self.root_folder, \"images\")\n",
    "            class_label = -1\n",
    "            for one_class in os.listdir(images_folder):\n",
    "                class_path = os.path.join(images_folder, one_class)\n",
    "                class_label += 1\n",
    "                print(\"img_classes path: \", class_path)\n",
    "                if os.path.isdir(class_path):\n",
    "                    n_images = 0\n",
    "                    for image in os.listdir(class_path):\n",
    "                        image_path = os.path.join(class_path, image)\n",
    "                        image_paths.append(image_path)\n",
    "                        n_images += 1\n",
    "                        # int\n",
    "                        classes_to_label.append(class_label)\n",
    "                        #print(image_path)\n",
    "                print(\"image appended: {}\".format(n_images))\n",
    "    \n",
    "            annotation_folder = os.path.join(self.root_folder, \"annotations\")\n",
    "            for one_class in os.listdir(annotation_folder):\n",
    "                class_path = os.path.join(annotation_folder, one_class)\n",
    "                print(\"annot_classes path: \", class_path)\n",
    "                if os.path.isdir(class_path):\n",
    "                    n_annotations = 0\n",
    "                    for annotation in os.listdir(class_path):\n",
    "                        annotation_path = os.path.join(class_path, annotation)\n",
    "                        annotation_paths.append(annotation_path)\n",
    "                        n_annotations += 1\n",
    "                        #print(annotation_path)\n",
    "                    print(\"annotation appended: {}\".format(n_annotations))\n",
    "    \n",
    "            # sort + np.array: os.listdir badly shuffles the names + annotations\n",
    "            image_paths.sort()\n",
    "            annotation_paths.sort()\n",
    "            image_paths = np.array(image_paths)\n",
    "            annotation_paths = np.array(annotation_paths)\n",
    "            print(\"CustomDataset initialized \\n\")\n",
    "            \n",
    "            return image_paths, classes_to_label, annotation_paths\n",
    "        else:\n",
    "            warnings.warn(\"No root folder specified. Ignore this warning, if you used '.train_test_split()'\")\n",
    "            return None, None, None\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        current_img_path = self.image_paths[idx]\n",
    "        current_annot_path = self.annotation_paths[idx]\n",
    "        current_class_label = self.class_label[idx]\n",
    "\n",
    "        # i have found colorless image -> must convert to 3d channel\n",
    "        current_img = PIL.Image.open(current_img_path).convert(\"RGB\")\n",
    "        current_annot = scipy.io.loadmat(current_annot_path)\n",
    "        current_annot = current_annot[\"box_coord\"].squeeze()\n",
    "        current_class_label = torch.tensor(current_class_label).float().unsqueeze(0)\n",
    "\n",
    "        # must transform annotation first (=bounding box), to get the original image sizes\n",
    "        # to keep the box accurate\n",
    "        if self.transform_annot:\n",
    "            current_annot = self.transform_annot(current_annot, current_img)\n",
    "        if self.transform_img:\n",
    "            current_img = self.transform_img(current_img)\n",
    "\n",
    "        #print(current_img_path)\n",
    "        return current_img, current_class_label, current_annot\n",
    "\n",
    "\n",
    "\n",
    "    def train_test_split(self, test_size):\n",
    "        \"\"\"\n",
    "        manual, basic split. random selection with no respect to class sizes.\n",
    "        \n",
    "        returns: new CustomDataset obj as test set.\n",
    "        note: returns with the same transforms as arguments on the original dataset\n",
    "        \n",
    "        generate random indexes, put those paths to the test_set, delete them from original paths\n",
    "        \"\"\"\n",
    "        total_test_size = int(len(self.image_paths) * test_size)\n",
    "        print(\"total test size: \", total_test_size)\n",
    "        rand_indexes = np.random.randint(0, len(self.image_paths), size=total_test_size)\n",
    "        rand_indexes = set(rand_indexes)\n",
    "        while len(rand_indexes) != total_test_size:\n",
    "            rand_indexes.add(np.random.randint(0, len(self.image_paths)))\n",
    "        # transform back to list for numpy to handle\n",
    "        rand_indexes = list(rand_indexes)\n",
    "        \n",
    "        test_image_paths = []\n",
    "        test_class_label = []\n",
    "        test_annotation_paths = []\n",
    "        for one_idx in rand_indexes:\n",
    "            test_image_paths.append(self.image_paths[one_idx])\n",
    "            test_class_label.append(self.class_label[one_idx])\n",
    "            test_annotation_paths.append(self.annotation_paths[one_idx])\n",
    "\n",
    "        self.image_paths = np.delete(self.image_paths, rand_indexes)\n",
    "        self.class_label = np.delete(self.class_label, rand_indexes)\n",
    "        self.annotation_paths = np.delete(self.annotation_paths, rand_indexes)\n",
    "\n",
    "        \"\"\"\n",
    "        OLD\n",
    "        test_set = []\n",
    "        # in case of when self.image_paths,... etc are simple lists. not np arrays\n",
    "        for elem in test_set:\n",
    "            if elem[0] in self.image_paths:\n",
    "                self.image_paths.remove(elem[0])\n",
    "            if elem[1] in self.class_label:\n",
    "                self.class_label.remove(elem[1])\n",
    "            if elem[2] in self.annotation_paths:\n",
    "                self.annotation_paths.remove(elem[2])\n",
    "        \"\"\"\n",
    "        new_dataset_obj = CustomDataset(root_folder=None, transform_img=self.transform_img, transform_annot=self.transform_annot)\n",
    "        new_dataset_obj.image_paths = test_image_paths\n",
    "        new_dataset_obj.class_label = test_class_label\n",
    "        new_dataset_obj.annotation_paths = test_annotation_paths\n",
    "        \n",
    "        return new_dataset_obj\n",
    "        \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "21962a4b-d588-486c-b851-0d1a5edcfa55",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CustomDataset initializing...\n",
      "img_classes path:  ./caltech-101/subset/images\\airplanes\n",
      "image appended: 800\n",
      "img_classes path:  ./caltech-101/subset/images\\Motorbikes\n",
      "image appended: 798\n",
      "annot_classes path:  ./caltech-101/subset/annotations\\airplanes\n",
      "annotation appended: 800\n",
      "annot_classes path:  ./caltech-101/subset/annotations\\Motorbikes\n",
      "annotation appended: 798\n",
      "CustomDataset initialized \n",
      "\n",
      "./caltech-101/subset/images\\Motorbikes\\image_0798.jpg\n",
      "0\n",
      "./caltech-101/subset/annotations\\Motorbikes\\annotation_0798.mat\n",
      "\n",
      "\n",
      "./caltech-101/subset/images\\airplanes\\image_0001.jpg\n",
      "0\n",
      "./caltech-101/subset/annotations\\airplanes\\annotation_0001.mat\n",
      "\n",
      "\n",
      "first elem: \n",
      " (<PIL.Image.Image image mode=RGB size=262x161 at 0x1FDD9C70D60>, tensor([0.]), array([ 19, 141,  31, 233], dtype=uint8)) \n",
      " ----------------------- \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# test dataset\n",
    "\n",
    "root_path = \"./caltech-101/subset/\"\n",
    "\n",
    "test_c_dataset = CustomDataset(root_path)\n",
    "\n",
    "print(test_c_dataset.image_paths[797])\n",
    "print(test_c_dataset.class_label[797])\n",
    "print(test_c_dataset.annotation_paths[797])\n",
    "print(\"\\n\")\n",
    "print(test_c_dataset.image_paths[798])\n",
    "print(test_c_dataset.class_label[798])\n",
    "print(test_c_dataset.annotation_paths[798])\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"first elem: \\n\", test_c_dataset[0], \"\\n\", \"-----------------------\",  \"\\n\")\n",
    "test_c_dataset[0][0].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7ce8b9a7-0b47-412a-bd22-71e5d941ff71",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CustomDataset initializing...\n",
      "img_classes path:  ./caltech-101/subset/images\\airplanes\n",
      "image appended: 800\n",
      "img_classes path:  ./caltech-101/subset/images\\Motorbikes\n",
      "image appended: 798\n",
      "annot_classes path:  ./caltech-101/subset/annotations\\airplanes\n",
      "annotation appended: 800\n",
      "annot_classes path:  ./caltech-101/subset/annotations\\Motorbikes\n",
      "annotation appended: 798\n",
      "CustomDataset initialized \n",
      "\n",
      "total test size:  159\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\neeko\\AppData\\Local\\Temp\\ipykernel_17904\\3024064077.py:65: UserWarning: No root folder specified. Ignore this warning, if you used '.train_test_split()'\n",
      "  warnings.warn(\"No root folder specified. Ignore this warning, if you used '.train_test_split()'\")\n"
     ]
    }
   ],
   "source": [
    "root_path = \"./caltech-101/subset/\"\n",
    "\n",
    "# !! got no idea how these function passes not results in error...\n",
    "train_set = CustomDataset(root_path, transform_img=preprocess_img, transform_annot=preprocess_bounding_box)\n",
    "test_set = train_set.train_test_split(0.1)\n",
    "loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "48736b3b-73c2-4f69-96ac-52d8f2ad6c92",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len train set:  1439\n",
      "len test set:  159\n",
      "\n",
      "./caltech-101/subset/images\\airplanes\\image_0078.jpg\n",
      "35. elem: \n",
      " (tensor([[[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.]]]), tensor([0.]), tensor([0.1221, 0.1221, 0.8779, 0.8626])) \n",
      " ----------------------- \n",
      "\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.])\n"
     ]
    }
   ],
   "source": [
    "# test loader\n",
    "\n",
    "print(\"len train set: \", len(train_set))\n",
    "print(\"len test set: \", len(test_set))\n",
    "print(\"\")\n",
    "\n",
    "print(train_set.image_paths[797])\n",
    "print(\"35. elem: \\n\", train_set[34], \"\\n\", \"-----------------------\",  \"\\n\")\n",
    "print(train_set[34][0][0][0])\n",
    "transform = transforms.ToPILImage()\n",
    "reconverted_img = transform(train_set[34][0])\n",
    "reconverted_img.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2111b077-8c87-42d3-ab7c-a2680b9957dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ac36bddc-3f98-45eb-b618-fb6716e3531b",
   "metadata": {},
   "source": [
    "## 6. training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9edfd2e9-6289-4358-ab1b-7d99c67b2309",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 for binary classif\n",
    "brain = Network(1)\n",
    "optimizer = Adam(brain.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f77fca86-0e82-4253-8478-454a51b51dca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  1 \t loss:  0.22491243\n",
      "epoch:  2 \t loss:  0.020246012\n",
      "epoch:  3 \t loss:  0.012180698\n",
      "epoch:  4 \t loss:  0.011309141\n",
      "epoch:  5 \t loss:  0.0079008825\n",
      "epoch:  6 \t loss:  0.0068750475\n",
      "epoch:  7 \t loss:  0.0056705046\n",
      "epoch:  8 \t loss:  0.005387346\n",
      "epoch:  9 \t loss:  0.0048338706\n",
      "epoch:  10 \t loss:  0.0044836593\n",
      "epoch:  11 \t loss:  0.0038560603\n",
      "epoch:  12 \t loss:  0.003512766\n",
      "epoch:  13 \t loss:  0.002778457\n",
      "epoch:  14 \t loss:  0.0027950117\n",
      "epoch:  15 \t loss:  0.0018151009\n",
      "epoch:  16 \t loss:  0.001189939\n",
      "epoch:  17 \t loss:  0.0010356811\n",
      "epoch:  18 \t loss:  0.00084402144\n",
      "epoch:  19 \t loss:  0.0006722611\n",
      "epoch:  20 \t loss:  0.0005182153\n"
     ]
    }
   ],
   "source": [
    "# training\n",
    "\n",
    "\n",
    "for epoch in range(1, n_epochs+1):\n",
    "    losses = []\n",
    "    for image, label, annotation in loader:\n",
    "        pred_class, pred_box = brain(image)\n",
    "        #print(pred_class)\n",
    "        #print(pred_box)\n",
    "        #print(label)\n",
    "        loss_classif = F.binary_cross_entropy(pred_class, label)\n",
    "        loss_box = F.mse_loss(pred_box, annotation)\n",
    "\n",
    "        # total loss?\n",
    "        total_loss = loss_classif + loss_box\n",
    "        losses.append(total_loss.data)\n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(\"epoch: \", epoch, \"\\t loss: \", np.mean(losses))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d88110-e772-454d-a156-25318d62e4a8",
   "metadata": {},
   "source": [
    "## 7. save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "22f9d1a5-dee0-4be9-8973-5990493830bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_output_txt():\n",
    "    structure = Network(1)\n",
    "    txt = f\"\"\"{structure.features}, \\n\n",
    "    flattened neurons: {structure.n_flattened_neurons}, \\n \n",
    "    classification head: {structure.class_head}, \\n\n",
    "    localization head: {structure.localize_head}, \\n\n",
    "    training batch size: {batch_size}, \n",
    "    learning rate: {learning_rate},\n",
    "    epochs: {n_epochs}\n",
    "    losses: {np.mean(losses)}\"\"\"\n",
    "    return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "df13ed57-3105-46cd-83b6-ec2738f2826f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved as:  plane_bike_checkpoint\n"
     ]
    }
   ],
   "source": [
    "base_output = \"outputs\"\n",
    "save_name = \"plane_bike_checkpoint\"\n",
    "save_file_extension = \".pth\"\n",
    "overwrite = True\n",
    "full_path = os.path.join(base_output, save_name + save_file_extension)\n",
    "\n",
    "\n",
    "os.makedirs(base_output, exist_ok=True)\n",
    "if os.path.exists(full_path):\n",
    "    if overwrite == True:\n",
    "        torch.save(brain.state_dict(), full_path)\n",
    "        print(\"saved as: \", save_name)\n",
    "    else:\n",
    "        print(\"save failed. file already exists. to overwrite, set overwrite=True\")\n",
    "else:\n",
    "    torch.save(brain.state_dict(), full_path)\n",
    "    print(\"saved as: \", save_name)\n",
    "\n",
    "with open(base_output + \"/\" + save_name + \".txt\", \"w\", encoding=\"utf-8\") as textfile:\n",
    "    textfile.write(prep_output_txt())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f35e9a8-a910-4ced-8ec8-711ddbac1823",
   "metadata": {},
   "source": [
    "## 8. testing + accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7f4afb3e-8e3c-414e-b36a-72b19d3436d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment to load the saved one.\n",
    "\n",
    "#brain.load_state_dict(torch.load(\"plane_bike_checkpoint.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "675daf3f-60f5-45cf-b36b-d72b4382701a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing\n",
    "\n",
    "# due to my mistake and lack of separate test folder, first i did not split the data to train-test.\n",
    "# so, i downloaded images from google to test.\n",
    "# i leave this here, because the result is interesting\n",
    "\n",
    "# 1. mass pred\n",
    "test_dataset = ImageFolder(\"./caltech-101/subset/manual_testset/\", transform=preprocess_img)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "print(\"test dataset: \", test_dataset, \"\\n\")\n",
    "\n",
    "brain.eval()\n",
    "predictions = []\n",
    "with torch.no_grad():\n",
    "    for img, label in test_loader:\n",
    "        pred_class, pred_box = brain(img)\n",
    "        predictions.append({\"class true\" : int(label), \"prediction prob\" : float(pred_class)})\n",
    "\n",
    "\n",
    "print(\"class true \\t prediction \\t prediction prob\")\n",
    "for elem in predictions:\n",
    "    if elem[\"class true\"] == 1:\n",
    "        elem[\"class true\"] = \"airplane\"\n",
    "    else:\n",
    "        elem[\"class true\"] = \"motorbike\"\n",
    "    if elem[\"prediction prob\"] >= 0.5:\n",
    "        elem[\"prediction\"] = \"airplane\"\n",
    "    else:\n",
    "        elem[\"prediction\"] = \"motorbike\"\n",
    "    print(elem[\"class true\"], \"\\t\", elem[\"prediction\"], \"\\t\", elem[\"prediction prob\"])\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "\n",
    "# 2. single pred\n",
    "img_path = \"./caltech-101/subset/manual_testset/motor/motor1.jpg\"\n",
    "image = PIL.Image.open(img_path)\n",
    "image.show()\n",
    "image = preprocess_img(image)\n",
    "image = image.unsqueeze(0)\n",
    "\n",
    "\n",
    "brain.eval()\n",
    "with torch.no_grad():\n",
    "    pred_class, pred_box = brain(image)\n",
    "\n",
    "print(\"single pred: \", pred_class)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8bf4af1a-2b39-470a-b9eb-909ab3a91cae",
   "metadata": {},
   "source": [
    "conclusion:\n",
    "\n",
    "the accuracy is garbage. images are way different than the training ones."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d43c05a9-3fc2-4948-9968-f56ccddc7c4a",
   "metadata": {},
   "source": [
    "## 8. testing2 (correctly?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c2e1a88e-fb20-4f56-afc2-2a6fee592ce5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class true \t prediction \t prediction prob \t bounding box\n",
      "0 \t\t 0 \t\t 0.000000 \t\t tensor([[0.1438, 0.1699, 0.8667, 0.8671]])\n",
      "0 \t\t 0 \t\t 0.000000 \t\t tensor([[0.1646, 0.2258, 0.8488, 0.8314]])\n",
      "0 \t\t 0 \t\t 0.000000 \t\t tensor([[0.1430, 0.1568, 0.8709, 0.8781]])\n",
      "0 \t\t 0 \t\t 0.000062 \t\t tensor([[0.1594, 0.2561, 0.8435, 0.7940]])\n",
      "1 \t\t 1 \t\t 0.999996 \t\t tensor([[0.1197, 0.1566, 0.8944, 0.8434]])\n",
      "0 \t\t 0 \t\t 0.000000 \t\t tensor([[0.1311, 0.1131, 0.8893, 0.8882]])\n",
      "1 \t\t 1 \t\t 0.999794 \t\t tensor([[0.1358, 0.1961, 0.8627, 0.8074]])\n",
      "1 \t\t 1 \t\t 0.999945 \t\t tensor([[0.1403, 0.1919, 0.8524, 0.8103]])\n",
      "1 \t\t 1 \t\t 0.999812 \t\t tensor([[0.1187, 0.2103, 0.8794, 0.8115]])\n",
      "1 \t\t 1 \t\t 0.999265 \t\t tensor([[0.1321, 0.2012, 0.8623, 0.7927]])\n",
      "0 \t\t 0 \t\t 0.000000 \t\t tensor([[0.1500, 0.2086, 0.8701, 0.8371]])\n",
      "1 \t\t 1 \t\t 0.999993 \t\t tensor([[0.1227, 0.1725, 0.8777, 0.8260]])\n",
      "1 \t\t 1 \t\t 0.998223 \t\t tensor([[0.1657, 0.2804, 0.8325, 0.7865]])\n",
      "1 \t\t 1 \t\t 0.892955 \t\t tensor([[0.1584, 0.3071, 0.8585, 0.7984]])\n",
      "0 \t\t 0 \t\t 0.000000 \t\t tensor([[0.1423, 0.1546, 0.8736, 0.8430]])\n",
      "1 \t\t 1 \t\t 0.999982 \t\t tensor([[0.1406, 0.2169, 0.8614, 0.8002]])\n",
      "0 \t\t 0 \t\t 0.000000 \t\t tensor([[0.1507, 0.1454, 0.8576, 0.8336]])\n",
      "0 \t\t 0 \t\t 0.000005 \t\t tensor([[0.1473, 0.1741, 0.8574, 0.8474]])\n",
      "0 \t\t 0 \t\t 0.000003 \t\t tensor([[0.1404, 0.1773, 0.8595, 0.8442]])\n",
      "0 \t\t 0 \t\t 0.000000 \t\t tensor([[0.1415, 0.1643, 0.8578, 0.8432]])\n",
      "0 \t\t 0 \t\t 0.000000 \t\t tensor([[0.1393, 0.1521, 0.8675, 0.8510]])\n",
      "1 \t\t 1 \t\t 0.999841 \t\t tensor([[0.1436, 0.2057, 0.8584, 0.8116]])\n",
      "1 \t\t 1 \t\t 0.999985 \t\t tensor([[0.1310, 0.1797, 0.8704, 0.8234]])\n",
      "0 \t\t 0 \t\t 0.000000 \t\t tensor([[0.1255, 0.1415, 0.8750, 0.8967]])\n",
      "0 \t\t 0 \t\t 0.000000 \t\t tensor([[0.1230, 0.1154, 0.8916, 0.9003]])\n",
      "0 \t\t 0 \t\t 0.000000 \t\t tensor([[0.1276, 0.1210, 0.8723, 0.8684]])\n",
      "1 \t\t 1 \t\t 0.999970 \t\t tensor([[0.1223, 0.2065, 0.8747, 0.8237]])\n",
      "0 \t\t 0 \t\t 0.000008 \t\t tensor([[0.1607, 0.1515, 0.8488, 0.7140]])\n",
      "1 \t\t 1 \t\t 0.999938 \t\t tensor([[0.1204, 0.2039, 0.8843, 0.8252]])\n",
      "1 \t\t 1 \t\t 0.999932 \t\t tensor([[0.1552, 0.2115, 0.8444, 0.8024]])\n",
      "0 \t\t 0 \t\t 0.000000 \t\t tensor([[0.1506, 0.1671, 0.8533, 0.8242]])\n",
      "0 \t\t 0 \t\t 0.000000 \t\t tensor([[0.1412, 0.2787, 0.8638, 0.8533]])\n",
      "0 \t\t 0 \t\t 0.000000 \t\t tensor([[0.1544, 0.2826, 0.8541, 0.8564]])\n",
      "0 \t\t 0 \t\t 0.000000 \t\t tensor([[0.1270, 0.1737, 0.8893, 0.8722]])\n",
      "1 \t\t 1 \t\t 0.999972 \t\t tensor([[0.1414, 0.2087, 0.8519, 0.7942]])\n",
      "0 \t\t 0 \t\t 0.016593 \t\t tensor([[0.1043, 0.1089, 0.8933, 0.8713]])\n",
      "0 \t\t 0 \t\t 0.000000 \t\t tensor([[0.1390, 0.1642, 0.8704, 0.8744]])\n",
      "1 \t\t 1 \t\t 0.999983 \t\t tensor([[0.1505, 0.2010, 0.8503, 0.8073]])\n",
      "0 \t\t 0 \t\t 0.000000 \t\t tensor([[0.1528, 0.2114, 0.8575, 0.8640]])\n",
      "0 \t\t 0 \t\t 0.000000 \t\t tensor([[0.1345, 0.1155, 0.8707, 0.8801]])\n",
      "1 \t\t 1 \t\t 0.999983 \t\t tensor([[0.1188, 0.2012, 0.8854, 0.8125]])\n",
      "0 \t\t 0 \t\t 0.000126 \t\t tensor([[0.1407, 0.1577, 0.8685, 0.8574]])\n",
      "0 \t\t 0 \t\t 0.000000 \t\t tensor([[0.1604, 0.1665, 0.8380, 0.8061]])\n",
      "0 \t\t 0 \t\t 0.000005 \t\t tensor([[0.1390, 0.1193, 0.8681, 0.8623]])\n",
      "0 \t\t 0 \t\t 0.000000 \t\t tensor([[0.1261, 0.1695, 0.8829, 0.8602]])\n",
      "1 \t\t 1 \t\t 0.999971 \t\t tensor([[0.1275, 0.1826, 0.8814, 0.8345]])\n",
      "1 \t\t 1 \t\t 0.999862 \t\t tensor([[0.1342, 0.1747, 0.8675, 0.8449]])\n",
      "1 \t\t 1 \t\t 0.999990 \t\t tensor([[0.1408, 0.1884, 0.8660, 0.8132]])\n",
      "1 \t\t 1 \t\t 0.999954 \t\t tensor([[0.1412, 0.2200, 0.8651, 0.8261]])\n",
      "0 \t\t 0 \t\t 0.000000 \t\t tensor([[0.1548, 0.1676, 0.8664, 0.8463]])\n",
      "1 \t\t 1 \t\t 0.999985 \t\t tensor([[0.1357, 0.1978, 0.8592, 0.8110]])\n",
      "0 \t\t 0 \t\t 0.000000 \t\t tensor([[0.1011, 0.0770, 0.8992, 0.8831]])\n",
      "0 \t\t 0 \t\t 0.000000 \t\t tensor([[0.1495, 0.2011, 0.8532, 0.8133]])\n",
      "0 \t\t 0 \t\t 0.000000 \t\t tensor([[0.1609, 0.1517, 0.8520, 0.8191]])\n",
      "1 \t\t 1 \t\t 0.999904 \t\t tensor([[0.1193, 0.1810, 0.8865, 0.8377]])\n",
      "0 \t\t 0 \t\t 0.000000 \t\t tensor([[0.1377, 0.1398, 0.8736, 0.8480]])\n",
      "0 \t\t 0 \t\t 0.000001 \t\t tensor([[0.1362, 0.2072, 0.8751, 0.8702]])\n",
      "1 \t\t 1 \t\t 0.940779 \t\t tensor([[0.1511, 0.2148, 0.8524, 0.8019]])\n",
      "1 \t\t 1 \t\t 0.999895 \t\t tensor([[0.1382, 0.2102, 0.8635, 0.8183]])\n",
      "1 \t\t 1 \t\t 0.999910 \t\t tensor([[0.1387, 0.2513, 0.8658, 0.8094]])\n",
      "0 \t\t 0 \t\t 0.000001 \t\t tensor([[0.1456, 0.2238, 0.8605, 0.8321]])\n",
      "1 \t\t 1 \t\t 0.999916 \t\t tensor([[0.1407, 0.1844, 0.8637, 0.8213]])\n",
      "1 \t\t 1 \t\t 0.999945 \t\t tensor([[0.1194, 0.2241, 0.8743, 0.8205]])\n",
      "0 \t\t 0 \t\t 0.000000 \t\t tensor([[0.1467, 0.1312, 0.8718, 0.8517]])\n",
      "1 \t\t 1 \t\t 0.999918 \t\t tensor([[0.1423, 0.2138, 0.8601, 0.8107]])\n",
      "0 \t\t 0 \t\t 0.000000 \t\t tensor([[0.1310, 0.1373, 0.8789, 0.8915]])\n",
      "0 \t\t 0 \t\t 0.000000 \t\t tensor([[0.1191, 0.0983, 0.8779, 0.8582]])\n",
      "1 \t\t 1 \t\t 0.999833 \t\t tensor([[0.1328, 0.1524, 0.8693, 0.8217]])\n",
      "0 \t\t 0 \t\t 0.000000 \t\t tensor([[0.1195, 0.1637, 0.8953, 0.8907]])\n",
      "0 \t\t 0 \t\t 0.000000 \t\t tensor([[0.1573, 0.2344, 0.8524, 0.8585]])\n",
      "1 \t\t 1 \t\t 0.999922 \t\t tensor([[0.1491, 0.1946, 0.8535, 0.7998]])\n",
      "0 \t\t 0 \t\t 0.000000 \t\t tensor([[0.1424, 0.2659, 0.8620, 0.8567]])\n",
      "0 \t\t 0 \t\t 0.000000 \t\t tensor([[0.1525, 0.1838, 0.8596, 0.7626]])\n",
      "0 \t\t 0 \t\t 0.000001 \t\t tensor([[0.1414, 0.1467, 0.8674, 0.8567]])\n",
      "0 \t\t 0 \t\t 0.000000 \t\t tensor([[0.1360, 0.1369, 0.8693, 0.8560]])\n",
      "0 \t\t 0 \t\t 0.000000 \t\t tensor([[0.1472, 0.1336, 0.8640, 0.8579]])\n",
      "1 \t\t 1 \t\t 0.999954 \t\t tensor([[0.1301, 0.1780, 0.8689, 0.8034]])\n",
      "1 \t\t 1 \t\t 0.999986 \t\t tensor([[0.1188, 0.1609, 0.8833, 0.8435]])\n",
      "0 \t\t 0 \t\t 0.000000 \t\t tensor([[0.1395, 0.1765, 0.8618, 0.8741]])\n",
      "0 \t\t 0 \t\t 0.053908 \t\t tensor([[0.1377, 0.1617, 0.8585, 0.8453]])\n",
      "0 \t\t 0 \t\t 0.000063 \t\t tensor([[0.1542, 0.1931, 0.8577, 0.8232]])\n",
      "0 \t\t 0 \t\t 0.000000 \t\t tensor([[0.1534, 0.1746, 0.8535, 0.8380]])\n",
      "1 \t\t 1 \t\t 0.999980 \t\t tensor([[0.1118, 0.1497, 0.8897, 0.8489]])\n",
      "0 \t\t 0 \t\t 0.000000 \t\t tensor([[0.1495, 0.2061, 0.8655, 0.8039]])\n",
      "0 \t\t 0 \t\t 0.000008 \t\t tensor([[0.1474, 0.2629, 0.8567, 0.8416]])\n",
      "1 \t\t 1 \t\t 0.999969 \t\t tensor([[0.1115, 0.1698, 0.8871, 0.8491]])\n",
      "1 \t\t 1 \t\t 0.999882 \t\t tensor([[0.1480, 0.1988, 0.8517, 0.8006]])\n",
      "0 \t\t 0 \t\t 0.000000 \t\t tensor([[0.1203, 0.1201, 0.8778, 0.8691]])\n",
      "0 \t\t 1 \t\t 0.999613 \t\t tensor([[0.1361, 0.1750, 0.8652, 0.8122]])\n",
      "1 \t\t 1 \t\t 0.999969 \t\t tensor([[0.1284, 0.2074, 0.8702, 0.8205]])\n",
      "1 \t\t 1 \t\t 0.999978 \t\t tensor([[0.1448, 0.1988, 0.8620, 0.8111]])\n",
      "1 \t\t 1 \t\t 0.999790 \t\t tensor([[0.1169, 0.2279, 0.8805, 0.8123]])\n",
      "1 \t\t 1 \t\t 0.999761 \t\t tensor([[0.1308, 0.2010, 0.8721, 0.8329]])\n",
      "1 \t\t 1 \t\t 0.999822 \t\t tensor([[0.1366, 0.2212, 0.8683, 0.7924]])\n",
      "1 \t\t 1 \t\t 0.999979 \t\t tensor([[0.1299, 0.1796, 0.8715, 0.8276]])\n",
      "1 \t\t 1 \t\t 0.751012 \t\t tensor([[0.1245, 0.1389, 0.8723, 0.8430]])\n",
      "1 \t\t 1 \t\t 0.999950 \t\t tensor([[0.1236, 0.2076, 0.8771, 0.8250]])\n",
      "1 \t\t 1 \t\t 0.999925 \t\t tensor([[0.1395, 0.1736, 0.8497, 0.8103]])\n",
      "1 \t\t 1 \t\t 0.999981 \t\t tensor([[0.1232, 0.1890, 0.8792, 0.7946]])\n",
      "1 \t\t 1 \t\t 0.974141 \t\t tensor([[0.1392, 0.2033, 0.8659, 0.8090]])\n",
      "1 \t\t 0 \t\t 0.032555 \t\t tensor([[0.1425, 0.1468, 0.8612, 0.8060]])\n",
      "1 \t\t 1 \t\t 0.999987 \t\t tensor([[0.1512, 0.2423, 0.8507, 0.7974]])\n",
      "1 \t\t 0 \t\t 0.410896 \t\t tensor([[0.1225, 0.1767, 0.8784, 0.8475]])\n",
      "0 \t\t 0 \t\t 0.000000 \t\t tensor([[0.1410, 0.1339, 0.8686, 0.8556]])\n",
      "0 \t\t 0 \t\t 0.000002 \t\t tensor([[0.1384, 0.1676, 0.8704, 0.8167]])\n",
      "0 \t\t 0 \t\t 0.000006 \t\t tensor([[0.1396, 0.1293, 0.8687, 0.8741]])\n",
      "0 \t\t 0 \t\t 0.000000 \t\t tensor([[0.1593, 0.1861, 0.8480, 0.8262]])\n",
      "1 \t\t 1 \t\t 0.999637 \t\t tensor([[0.1303, 0.2373, 0.8691, 0.8090]])\n",
      "0 \t\t 0 \t\t 0.000001 \t\t tensor([[0.1443, 0.1823, 0.8658, 0.7857]])\n",
      "1 \t\t 1 \t\t 0.999960 \t\t tensor([[0.1256, 0.2004, 0.8802, 0.8133]])\n",
      "1 \t\t 1 \t\t 0.999880 \t\t tensor([[0.1371, 0.1968, 0.8633, 0.7999]])\n",
      "0 \t\t 0 \t\t 0.000003 \t\t tensor([[0.1491, 0.1471, 0.8571, 0.8637]])\n",
      "0 \t\t 0 \t\t 0.000009 \t\t tensor([[0.1443, 0.1309, 0.8588, 0.8689]])\n",
      "1 \t\t 1 \t\t 0.999993 \t\t tensor([[0.1329, 0.1874, 0.8681, 0.8133]])\n",
      "1 \t\t 1 \t\t 0.999811 \t\t tensor([[0.1369, 0.1726, 0.8679, 0.7972]])\n",
      "0 \t\t 0 \t\t 0.000004 \t\t tensor([[0.1586, 0.1978, 0.8582, 0.8137]])\n",
      "1 \t\t 1 \t\t 0.999615 \t\t tensor([[0.1465, 0.3639, 0.8554, 0.7519]])\n",
      "1 \t\t 1 \t\t 0.999902 \t\t tensor([[0.1374, 0.1977, 0.8560, 0.7879]])\n",
      "1 \t\t 1 \t\t 0.999988 \t\t tensor([[0.1458, 0.1763, 0.8658, 0.8121]])\n",
      "1 \t\t 1 \t\t 0.999469 \t\t tensor([[0.1386, 0.1737, 0.8612, 0.7886]])\n",
      "1 \t\t 1 \t\t 0.999940 \t\t tensor([[0.1402, 0.1731, 0.8602, 0.7927]])\n",
      "1 \t\t 1 \t\t 0.999987 \t\t tensor([[0.1298, 0.1524, 0.8674, 0.8152]])\n",
      "1 \t\t 1 \t\t 0.999970 \t\t tensor([[0.1068, 0.2101, 0.8879, 0.8026]])\n",
      "1 \t\t 1 \t\t 0.999988 \t\t tensor([[0.1450, 0.1987, 0.8566, 0.8078]])\n",
      "1 \t\t 1 \t\t 0.996711 \t\t tensor([[0.1717, 0.2176, 0.8319, 0.7934]])\n",
      "1 \t\t 1 \t\t 0.999993 \t\t tensor([[0.1411, 0.1571, 0.8625, 0.8274]])\n",
      "1 \t\t 1 \t\t 0.999419 \t\t tensor([[0.1430, 0.2392, 0.8561, 0.7897]])\n",
      "1 \t\t 1 \t\t 0.999990 \t\t tensor([[0.1215, 0.1682, 0.8801, 0.8265]])\n",
      "0 \t\t 0 \t\t 0.000000 \t\t tensor([[0.1570, 0.1696, 0.8536, 0.8399]])\n",
      "1 \t\t 1 \t\t 0.999967 \t\t tensor([[0.1381, 0.2150, 0.8639, 0.7853]])\n",
      "1 \t\t 1 \t\t 0.999835 \t\t tensor([[0.1251, 0.2082, 0.8809, 0.7935]])\n",
      "0 \t\t 0 \t\t 0.000001 \t\t tensor([[0.1797, 0.1594, 0.8366, 0.8205]])\n",
      "1 \t\t 1 \t\t 0.973059 \t\t tensor([[0.1271, 0.1658, 0.8719, 0.8211]])\n",
      "0 \t\t 0 \t\t 0.000000 \t\t tensor([[0.1513, 0.1430, 0.8646, 0.8459]])\n",
      "1 \t\t 1 \t\t 0.999867 \t\t tensor([[0.1382, 0.2016, 0.8671, 0.8099]])\n",
      "0 \t\t 0 \t\t 0.000000 \t\t tensor([[0.1481, 0.1595, 0.8645, 0.8385]])\n",
      "1 \t\t 1 \t\t 0.999948 \t\t tensor([[0.1392, 0.1773, 0.8698, 0.8181]])\n",
      "1 \t\t 1 \t\t 0.999967 \t\t tensor([[0.1410, 0.2098, 0.8657, 0.8024]])\n",
      "1 \t\t 1 \t\t 0.999988 \t\t tensor([[0.1237, 0.1966, 0.8752, 0.8110]])\n",
      "1 \t\t 1 \t\t 0.999442 \t\t tensor([[0.1293, 0.2713, 0.8702, 0.7876]])\n",
      "1 \t\t 1 \t\t 0.999768 \t\t tensor([[0.1305, 0.2047, 0.8693, 0.8206]])\n",
      "1 \t\t 1 \t\t 0.999133 \t\t tensor([[0.1327, 0.1772, 0.8676, 0.8152]])\n",
      "0 \t\t 0 \t\t 0.000000 \t\t tensor([[0.1521, 0.1704, 0.8614, 0.8432]])\n",
      "1 \t\t 1 \t\t 0.999828 \t\t tensor([[0.1243, 0.1893, 0.8764, 0.8157]])\n",
      "1 \t\t 1 \t\t 0.999969 \t\t tensor([[0.1251, 0.2006, 0.8701, 0.8167]])\n",
      "0 \t\t 0 \t\t 0.000000 \t\t tensor([[0.1557, 0.1598, 0.8583, 0.8435]])\n",
      "1 \t\t 1 \t\t 0.999981 \t\t tensor([[0.1330, 0.1843, 0.8719, 0.8373]])\n",
      "0 \t\t 0 \t\t 0.000000 \t\t tensor([[0.1390, 0.1473, 0.8672, 0.8427]])\n",
      "1 \t\t 1 \t\t 0.999703 \t\t tensor([[0.1403, 0.2517, 0.8585, 0.7811]])\n",
      "1 \t\t 1 \t\t 0.999989 \t\t tensor([[0.1217, 0.1775, 0.8777, 0.8226]])\n",
      "1 \t\t 1 \t\t 0.999811 \t\t tensor([[0.1193, 0.1661, 0.8773, 0.8196]])\n",
      "0 \t\t 0 \t\t 0.000000 \t\t tensor([[0.1442, 0.1759, 0.8609, 0.8346]])\n",
      "1 \t\t 1 \t\t 0.999561 \t\t tensor([[0.1398, 0.2609, 0.8555, 0.7897]])\n",
      "1 \t\t 1 \t\t 0.999942 \t\t tensor([[0.1021, 0.1610, 0.8941, 0.8522]])\n",
      "1 \t\t 1 \t\t 0.999986 \t\t tensor([[0.1254, 0.1594, 0.8756, 0.8207]])\n",
      "1 \t\t 1 \t\t 0.999817 \t\t tensor([[0.1304, 0.2110, 0.8711, 0.8078]])\n",
      "1 \t\t 1 \t\t 0.999781 \t\t tensor([[0.1318, 0.2009, 0.8640, 0.8375]])\n",
      "0 \t\t 0 \t\t 0.000002 \t\t tensor([[0.1502, 0.2089, 0.8627, 0.8347]])\n",
      "1 \t\t 1 \t\t 0.999947 \t\t tensor([[0.1243, 0.2000, 0.8682, 0.7915]])\n"
     ]
    }
   ],
   "source": [
    "# testing\n",
    "\n",
    "test_loader = DataLoader(test_set, batch_size=1, shuffle=False)\n",
    "predictions = []\n",
    "\n",
    "brain.eval()\n",
    "with torch.no_grad():\n",
    "    for img, label, annotation in test_loader:\n",
    "        pred_class, pred_box = brain(img)\n",
    "        predictions.append({\"class true\" : int(label),\n",
    "                            \"prediction prob\" : float(pred_class),\n",
    "                            \"bounding_box\" : pred_box})\n",
    "\n",
    "\n",
    "print(\"class true \\t prediction \\t prediction prob \\t bounding box\")\n",
    "for elem in predictions:\n",
    "    \"\"\"\n",
    "    if elem[\"class true\"] == 1:\n",
    "        elem[\"class true\"] = \"airplane\"\n",
    "    else:\n",
    "        elem[\"class true\"] = \"motorbike\"\n",
    "    \"\"\"\n",
    "    if elem[\"prediction prob\"] >= 0.5:\n",
    "        elem[\"prediction\"] = 1\n",
    "    else:\n",
    "        elem[\"prediction\"] = 0\n",
    "    if elem[\"prediction prob\"]:\n",
    "        elem[\"prediction prob\"] = \"{:.6f}\".format(elem[\"prediction prob\"])\n",
    "    print(elem[\"class true\"], \"\\t\\t\", elem[\"prediction\"], \"\\t\\t\", elem[\"prediction prob\"], \"\\t\\t\", elem[\"bounding_box\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "9e91673c-b3bc-4cde-95f3-6545000515d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sklearn accuracy:  0.9811320754716981\n",
      "np accuracy:  0.9811320754716981\n"
     ]
    }
   ],
   "source": [
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "for elem in predictions:\n",
    "    y_true.append(elem[\"class true\"])\n",
    "    y_pred.append(elem[\"prediction\"])\n",
    "print(\"sklearn accuracy: \", accuracy_score(y_true, y_pred))\n",
    "\n",
    "np_acc = np.sum(np.equal(y_true, y_pred)) / len(y_true)\n",
    "print(\"np accuracy: \", np_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ffd5cd6f-e667-402f-b855-1b6f4d56b061",
   "metadata": {},
   "outputs": [],
   "source": [
    "# too high accuracy. -> overfitting?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "85e10db3-4e7e-4b1e-bb8a-03bfc74eb5a7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rectangle true:  [30, 15, 231, 132]\n",
      "rectangle pred:  [37, 25, 227, 130]\n",
      "class pred:  0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rectangle true:  [44, 44, 209, 141]\n",
      "rectangle pred:  [42, 38, 221, 142]\n",
      "class pred:  0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rectangle true:  [36, 22, 226, 129]\n",
      "rectangle pred:  [37, 23, 227, 130]\n",
      "class pred:  0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rectangle true:  [35, 41, 223, 137]\n",
      "rectangle pred:  [41, 45, 220, 142]\n",
      "class pred:  0.0001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rectangle true:  [58, 30, 357, 151]\n",
      "rectangle pred:  [49, 28, 370, 152]\n",
      "class pred:  1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rectangle true:  [36, 23, 230, 136]\n",
      "rectangle pred:  [34, 17, 232, 138]\n",
      "class pred:  0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rectangle true:  [50, 32, 348, 145]\n",
      "rectangle pred:  [54, 34, 345, 142]\n",
      "class pred:  0.9998\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rectangle true:  [65, 33, 356, 148]\n",
      "rectangle pred:  [58, 35, 357, 149]\n",
      "class pred:  0.9999\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[66], line 42\u001b[0m\n\u001b[0;32m     40\u001b[0m img_orig\u001b[38;5;241m.\u001b[39mshow()\n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m# wait for buttonpress\u001b[39;00m\n\u001b[1;32m---> 42\u001b[0m \u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mE:\\programozas\\machine_learning\\udemy_machinelearn+old_tensorflow_basics\\.venv\\lib\\site-packages\\ipykernel\\kernelbase.py:1282\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[1;34m(self, prompt)\u001b[0m\n\u001b[0;32m   1280\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1281\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[1;32m-> 1282\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_input_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1283\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1284\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parent_ident\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1285\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_parent\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1286\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1287\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mE:\\programozas\\machine_learning\\udemy_machinelearn+old_tensorflow_basics\\.venv\\lib\\site-packages\\ipykernel\\kernelbase.py:1325\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[1;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[0;32m   1322\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[0;32m   1323\u001b[0m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[0;32m   1324\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1325\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1326\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1327\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "# check bounding box\n",
    "\n",
    "\n",
    "def check_bounding_box():\n",
    "    pass\n",
    "    \n",
    "for img_path, annotation_path in zip(test_set.image_paths, test_set.annotation_paths):\n",
    "    #raise Exception(\"bounding box might be good, but need to figure out the correct rectangle drawing. see above at preprocessing\")\n",
    "    # use cv2 to draw the rectangle\n",
    "    #cv_img = cv2.imread(img_path)\n",
    "    \n",
    "    img_orig = PIL.Image.open(img_path)\n",
    "    img = PIL.Image.open(img_path).convert(\"RGB\")\n",
    "    annot = scipy.io.loadmat(annotation_path)\n",
    "    annot = annot[\"box_coord\"].squeeze()\n",
    "    print(\"rectangle true: \", [annot[2], annot[0], annot[3], annot[1]])\n",
    "    annot = preprocess_bounding_box(annot, img)\n",
    "    \n",
    "    # get aspect ratio\n",
    "    w, h = img.size\n",
    "    img = preprocess_img(img).unsqueeze(0)\n",
    "\n",
    "    # predict\n",
    "    brain.eval()\n",
    "    with torch.no_grad():\n",
    "        pred_class, pred_box = brain(img)\n",
    "\n",
    "    pred_box = pred_box.squeeze()\n",
    "    #print(pred_box)\n",
    "    x1 = int(pred_box[0] * w)\n",
    "    y1 = int(pred_box[1] * h)\n",
    "    x2 = int(pred_box[2] * w)\n",
    "    y2 = int(pred_box[3] * h)\n",
    "\n",
    "    # draw rectangle on img\n",
    "    print(\"rectangle pred: \", [x1, y1, x2, y2])\n",
    "    print(\"class pred: \", f\"{float(pred_class):.4f}\")\n",
    "    draw_obj = PIL.ImageDraw.Draw(img_orig)\n",
    "    draw_obj.rectangle([x1, y1, x2, y2], outline=\"red\")\n",
    "    img_orig.show()\n",
    "    # wait for buttonpress\n",
    "    input(\"\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b40899-8650-4481-8754-7c1786028fc6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
